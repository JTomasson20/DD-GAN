{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a0f17d6",
   "metadata": {},
   "source": [
    "# Original code\n",
    "\n",
    "Below is the code originally given to us by Claire, execute it to see that the code that the results of the original version correspond to the results of the old version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982a9153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import sklearn.utils\n",
    "import sklearn.preprocessing\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "seed = 143\n",
    "\n",
    "initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05,\n",
    "                                                 seed=seed)\n",
    "\n",
    "def make_generator(nsteps):\n",
    "    generator = tf.keras.Sequential()\n",
    "    generator.add(tf.keras.layers.Dense(5, input_shape=(5,), activation='relu', kernel_initializer=initializer)) # 5\n",
    "    generator.add(tf.keras.layers.BatchNormalization())\n",
    "    generator.add(tf.keras.layers.Dense(10, activation='relu', kernel_initializer=initializer))             # 10\n",
    "    generator.add(tf.keras.layers.BatchNormalization())\n",
    "    generator.add(tf.keras.layers.Dense((5*nsteps), activation='relu', kernel_initializer=initializer))          # 25\n",
    "    generator.add(tf.keras.layers.BatchNormalization())\n",
    "    generator.add(tf.keras.layers.Dense((5*nsteps), activation='tanh', kernel_initializer=initializer))          # 25\n",
    "\n",
    "    return generator\n",
    "\n",
    "\n",
    "def make_critic(nsteps):\n",
    "    discriminator = tf.keras.Sequential()\n",
    "    discriminator.add(tf.keras.layers.Dense(5*nsteps, input_shape=(5*nsteps,), kernel_initializer=initializer))\n",
    "    discriminator.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "    discriminator.add(tf.keras.layers.Dropout(0.3))\n",
    "    discriminator.add(tf.keras.layers.Dense(10, kernel_initializer=initializer))\n",
    "    discriminator.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "    discriminator.add(tf.keras.layers.Dense(5, kernel_initializer=initializer))\n",
    "    discriminator.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "    discriminator.add(tf.keras.layers.Dropout(0.3))\n",
    "    discriminator.add(tf.keras.layers.Flatten())\n",
    "    discriminator.add(tf.keras.layers.Dense(1, kernel_initializer=initializer))\n",
    "\n",
    "\n",
    "    return discriminator\n",
    "\n",
    "\n",
    "def discriminator_loss(d_real, d_fake):\n",
    "    d_loss = tf.reduce_mean(d_fake) - tf.reduce_mean(d_real)\n",
    "    return d_loss\n",
    "\n",
    "def generator_loss(d_fake):\n",
    "    g_loss = -tf.reduce_mean(d_fake)\n",
    "    return g_loss\n",
    "\n",
    "@tf.function\n",
    "def train_step(noise, real, lmbda, n_critic, g, c, g_opt, c_opt, g1_loss, d1_loss, w_loss):   \n",
    "    batch_size = len(real)\n",
    "\n",
    "    ##### train critic ######\n",
    "    \n",
    "    #print('inside train step') \n",
    "\n",
    "    for i in range(n_critic):    \n",
    "        with tf.GradientTape() as t:\n",
    "            with tf.GradientTape() as t1:\n",
    "                fake = g(noise, training=True) # training=False?\n",
    "                # tf.print(fake)\n",
    "                epsilon = tf.random.uniform(shape=[batch_size, 1], minval=0., maxval=1.)\n",
    "                #print ('types', real.dtype, epsilon.dtype, fake.dtype)                \n",
    "                interpolated = real + epsilon * (fake - real) \n",
    "                t1.watch(interpolated)\n",
    "                c_inter = c(interpolated, training=True)  \n",
    "                d_real = c(real, training=True)\n",
    "                d_fake = c(fake, training=True)\n",
    "                d_loss = discriminator_loss(d_real, d_fake)   #initial c loss\n",
    "                \n",
    "                #print('d loop', d_real.shape,  d_fake.shape,  d_loss.shape, c_inter.shape) # batch_size by 1 except for d_loss\n",
    "                #print('d loop',  interpolated.shape, inpt.shape, real.shape, fake.shape) # \n",
    "                \n",
    "                \n",
    "                #print('discriminator loop',i ,'---------------------------------------------')    \n",
    "                #print('min and max values of fake',np.min(fake.numpy()),np.max(fake.numpy()))\n",
    "                #print('min and max values of real',np.min(real),np.max(real) )\n",
    "                #print('min and max values of d_fake',np.min(d_fake.numpy()),np.max(d_fake.numpy()))\n",
    "                #print('min and max values of d_real',np.min(d_real.numpy()),np.max(d_real.numpy()))\n",
    "                \n",
    "            grad_interpolated = t1.gradient(c_inter, interpolated)\n",
    "            \n",
    "            #print('interpolated      ', interpolated.numpy())\n",
    "            #print('c_inter           ', c_inter.numpy())\n",
    "            #print('grad_interpolated', grad_interpolated.numpy())\n",
    "            \n",
    "            #print('grad_interpolated itself ', grad_interpolated.numpy().shape) # batch_size by 25\n",
    "            #print('grad_interpolated square ', tf.square(grad_interpolated).numpy().shape) # batch_size by 25\n",
    "            #print('grad_interpolated red sum', tf.reduce_sum(tf.square(grad_interpolated), axis=[1]).numpy().shape) # batch_size by 1\n",
    "            #print('grad_interpolated sqrt', tf.sqrt(tf.reduce_sum(tf.square(grad_interpolated), axis=[1])).numpy().shape) # batch_size by 1            \n",
    "            \n",
    "                     #tf.sqrt(tf.reduce_sum(tf.square(x)) + 1.0e-12)\n",
    "            slopes = tf.sqrt(tf.reduce_sum(tf.square(grad_interpolated) + 1e-12, axis=[1])) # \n",
    "            \n",
    "            gradient_penalty = tf.reduce_mean((slopes - 1.) ** 2)\n",
    "            #print('slopes, grad penalty', slopes.numpy(), gradient_penalty.numpy())\n",
    "\n",
    "            new_d_loss = d_loss + (lmbda*gradient_penalty)  #new c loss\n",
    "            #print('d_loss and new_d_loss',d_loss.numpy(), new_d_loss.numpy()) \n",
    "        \n",
    "        c_grad = t.gradient(new_d_loss, c.trainable_variables)\n",
    "        #print('length and type c_grad', len(c_grad), type(c_grad))  # length of c_grad was 8? type list\n",
    "        #print('length and type c_grad[0]', (c_grad[0].shape), type(c_grad[0]))  # length of c_grad was 8? type list\n",
    "        #print('values of cgrad',c_grad)\n",
    "        #print('c.trainable_variables', c.trainable_variables)\n",
    "        c_opt.apply_gradients(zip(c_grad, c.trainable_variables))\n",
    "\n",
    "\n",
    "    ##### train generator #####\n",
    "\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        fake_images = g(noise, training=True)\n",
    "        d_fake = c(fake_images, training=True) # training=False?\n",
    "        g_loss = generator_loss(d_fake)\n",
    "\n",
    "    gen_grads = gen_tape.gradient(g_loss, g.trainable_variables)\n",
    "    g_opt.apply_gradients(zip(gen_grads, g.trainable_variables))\n",
    "    \n",
    "    \n",
    "    #print('g.trainable_variables')\n",
    "    #for v in g.trainable_variables:\n",
    "    #  print(v.name)\n",
    "    #print('c.trainable_variables')\n",
    "    #for v in c.trainable_variables:\n",
    "    #  print(v.name)\n",
    "\n",
    "    ### for tensorboard\n",
    "    g1_loss(g_loss)\n",
    "    d1_loss(new_d_loss)\n",
    "    w_loss((-1)*(d_loss))  #wasserstein distance\n",
    "\n",
    "\n",
    "    return \n",
    "\n",
    "def train(nsteps,ndims,lmbda,n_critic,batch_size,batches,training_data,input_to_GAN, epochs, g, c, g_opt, c_opt, g1_loss, d1_loss, w_loss, g1_summary_writer, d1_summary_writer, w_summary_writer):\n",
    "\n",
    "    losses = np.zeros((epochs,4))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        noise = input_to_GAN\n",
    "        real_data = training_data #X1.astype('int')\n",
    "\n",
    "        # uncommenting this line means that the noise is not paired with the outputs (probably desirable)\n",
    "        #noise = np.random.normal(size=[noise.shape[0],noise.shape[1]])\n",
    " \n",
    "        real_data, noise = sklearn.utils.shuffle(real_data, noise) #shuffle each epoch\n",
    "        #print ('shuffled l_input1 and xn1_comp')\n",
    "        \n",
    "        xx1 = real_data.reshape(batches, batch_size, ndims*nsteps)\n",
    "        inpt1 = noise.reshape(batches, batch_size, ndims)\n",
    "        #print ('data arranged in batches')\n",
    "        \n",
    "        #print(g.layers[0].weights)\n",
    "\n",
    "        for i in range(len(xx1)):\n",
    "            #print('calling train_step', i ,'of',len(xx1), '-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*')  \n",
    "            train_step(inpt1[i], xx1[i], lmbda, n_critic, g, c, g_opt, c_opt, g1_loss, d1_loss, w_loss)\n",
    "            #print('back from train_step')  \n",
    "\n",
    "        print('epoch:', epoch, '*************************************************')    \n",
    "        print('gen loss', g1_loss.result().numpy(), 'd loss', d1_loss.result().numpy(), 'w_loss' , w_loss.result().numpy())\n",
    "\n",
    "        losses[epoch,:] = [ epoch+1, g1_loss.result().numpy() ,   d1_loss.result().numpy(),  w_loss.result().numpy()]\n",
    "\n",
    "        with g1_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', g1_loss.result(), step=epoch)\n",
    "\n",
    "        with d1_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', d1_loss.result(), step=epoch)\n",
    "\n",
    "        with w_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', w_loss.result(), step=epoch)\n",
    "            \n",
    "        #print('reset states')\n",
    "        g1_loss.reset_states()\n",
    "        d1_loss.reset_states()\n",
    "        w_loss.reset_states()\n",
    "\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "        #if epoch < 100 or (epoch + 1) % 100 == 0 :\n",
    "                       \n",
    "            saved_g1_dir = './saved_g_' + str(epoch + 1)\n",
    "            saved_d1_dir = './saved_c_' + str(epoch + 1)\n",
    "            tf.keras.models.save_model(g, saved_g1_dir)\n",
    "            tf.keras.models.save_model(c, saved_d1_dir)\n",
    "\n",
    "    np.savetxt('losses.csv', losses, delimiter=',')\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "def learn_hypersurface_from_POD_coeffs(input_to_GAN, training_data, nsteps, nPOD, ndims, lmbda, n_critic, batch_size, batches, ndims_latent_input):\n",
    "    # nPOD not needed\n",
    "\n",
    "    try:\n",
    "      print('looking for previous saved models')\n",
    "      saved_g1_dir = './saved_g_' + str(model_number)\n",
    "      g = tf.keras.models.load_model(saved_g1_dir)\n",
    "\n",
    "      saved_d1_dir = './saved_c_' + str(model_number)\n",
    "      c = tf.keras.models.load_model(saved_d1_dir)\n",
    "\n",
    "\n",
    "    except:\n",
    "      print('making new generator and critic')\n",
    "      g = make_generator(nsteps)\n",
    "      c = make_critic(nsteps)\n",
    "\n",
    "\n",
    "    g_opt = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0, beta_2=0.9)\n",
    "    c_opt = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0, beta_2=0.9)\n",
    "\n",
    "    g1_loss = tf.keras.metrics.Mean('g1_loss', dtype=tf.float32)\n",
    "    d1_loss = tf.keras.metrics.Mean('d1_loss', dtype=tf.float32)\n",
    "    w_loss = tf.keras.metrics.Mean('w_loss', dtype=tf.float32)\n",
    "\n",
    "    # logs to follow losses on tensorboard\n",
    "    print('initialising logs for tensorboard')\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    g1_log_dir = './logs/gradient_tape/' + current_time + '/g'\n",
    "    d1_log_dir = './logs/gradient_tape/' + current_time + '/d'\n",
    "    w_log_dir = './logs/gradient_tape/' + current_time + '/w'\n",
    "\n",
    "    g1_summary_writer = tf.summary.create_file_writer(g1_log_dir)\n",
    "    d1_summary_writer = tf.summary.create_file_writer(d1_log_dir)\n",
    "    w_summary_writer = tf.summary.create_file_writer(w_log_dir)\n",
    "\n",
    "\n",
    "    print('beginning training')\n",
    "    epochs = 20\n",
    "    generator = train(nsteps,ndims,lmbda,n_critic,batch_size,batches,training_data,input_to_GAN, epochs, g, c, g_opt, c_opt, g1_loss, d1_loss, w_loss, g1_summary_writer, d1_summary_writer, w_summary_writer)\n",
    "    print('ending training')\n",
    "\n",
    "\n",
    "    # generate some random inputs and put through generator\n",
    "    number_test_examples = 10\n",
    "    test_input = tf.random.normal([number_test_examples, ndims_latent_input])\n",
    "    predictions = generator(test_input, training=False)\n",
    "    #predictions = generator.predict(test_input) # number_test_examples by ndims_latent_input\n",
    "\n",
    "    predictions_np = predictions.numpy() # nExamples by nPOD*nsteps\n",
    "    #tf.compat.v1.InteractiveSession()\n",
    "    #predictions_np = predictions.numpy().\n",
    "    print('Shape of the output of the GAN', predictions_np.shape)\n",
    "    predictions_np = predictions_np.reshape(number_test_examples*nsteps, nPOD)\n",
    "    print('Reshaping the GAN output (in order to apply inverse scaling)', predictions_np.shape)\n",
    "\n",
    "    return predictions_np, generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92f9fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in the POD coeffs.\n",
      "type and shape (nPOD by nTrain) of POD coeffs from csv file <class 'numpy.ndarray'> (5, 204) float32\n",
      "shape csv data for the scaling (204, 5)\n",
      "min and max of col,  0  of csv_data: -1.0 1.0\n",
      "min and max of col,  1  of csv_data: -1.0 1.0000001\n",
      "min and max of col,  2  of csv_data: -1.0 1.0\n",
      "min and max of col,  3  of csv_data: -1.0 1.0\n",
      "min and max of col,  4  of csv_data: -1.0 1.0\n",
      "Shape of training data for the GAN (200, 25) float32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "# reproducibility\n",
    "import random\n",
    "\n",
    "np.random.seed(143)\n",
    "random.seed(143)\n",
    "tf.random.set_seed(143)\n",
    "\n",
    "# read in data, reshape and normalise\n",
    "lmbda = 10\n",
    "n_critic = 5\n",
    "\n",
    "batch_size = 20 # 32  \n",
    "batches = 10    # 900 \n",
    "\n",
    "ndims_latent_input = 5 # 128 # latent variables for GAN\n",
    "\n",
    "# data settings\n",
    "nsteps = 5  #number of consecutive timesteps in gan\n",
    "ndims = 5 # == nPOD # 128 # reduced variables i.e. POD coefficients or AE latent variables\n",
    "\n",
    "# reading in the data\n",
    "print('Reading in the POD coeffs.')\n",
    "#csv_data = np.loadtxt('/kaggle/input/fpc-204examples-5pod-without-ic/POD_coeffs_1_204.csv', delimiter=',')\n",
    "csv_data = np.loadtxt('../data/processed/POD_coeffs_1_204_orig.csv', delimiter=',')\n",
    "csv_data = np.float32(csv_data)\n",
    "print('type and shape (nPOD by nTrain) of POD coeffs from csv file', type(csv_data), csv_data.shape, csv_data.dtype)\n",
    "\n",
    "nTrain = csv_data.shape[1]\n",
    "nPOD = csv_data.shape[0]\n",
    "\n",
    "csv_data = csv_data.T # nTrain by nPOD\n",
    "\n",
    "# scaling the POD coeffs\n",
    "scaling = sklearn.preprocessing.MinMaxScaler(feature_range=[-1,1])\n",
    "print('shape csv data for the scaling', csv_data.shape) \n",
    "csv_data = scaling.fit_transform(csv_data)\n",
    "\n",
    "# check that the columns are scaled between min and max values (-1,1)\n",
    "for icol in range(csv_data.shape[1]):\n",
    "    print('min and max of col, ', icol ,' of csv_data:', np.min(csv_data[:,icol]), np.max(csv_data[:,icol]) )\n",
    "\n",
    "# create nsteps time levels for the training_data for the GAN\n",
    "t_begin = 0\n",
    "t_end = nTrain - nsteps + 1\n",
    "training_data = np.zeros((t_end,nPOD*nsteps),dtype=np.float32) # nTrain by nsteps*nPOD # 'float32' or np.float32\n",
    "\n",
    "for step in range(nsteps):\n",
    "    #print ('training data - cols',step*nPOD,'to',(step+1)*nPOD )\n",
    "    #print ('csv data - rows', t_begin+step ,'to', t_end+step )\n",
    "    training_data[:,step*nPOD:(step+1)*nPOD] = csv_data[t_begin+step : t_end+step,:]\n",
    "\n",
    "print('Shape of training data for the GAN', training_data.shape, training_data.dtype)\n",
    "\n",
    "# GAN input\n",
    "try:\n",
    "    input_to_GAN = np.load('input_to_GAN.npy')\n",
    "except:\n",
    "    input_to_GAN = tf.random.normal([training_data.shape[0], ndims_latent_input])\n",
    "    input_to_GAN = input_to_GAN.numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c7eb718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking for previous saved models\n",
      "making new generator and critic\n",
      "initialising logs for tensorboard\n",
      "beginning training\n",
      "epoch: 0 *************************************************\n",
      "gen loss 0.00044128043 d loss 9.975678 w_loss 0.00051889766\n",
      "epoch: 1 *************************************************\n",
      "gen loss 0.0014283005 d loss 9.944265 w_loss 0.0013156604\n",
      "epoch: 2 *************************************************\n",
      "gen loss 0.0023684455 d loss 9.915084 w_loss 0.0020202943\n",
      "epoch: 3 *************************************************\n",
      "gen loss 0.0036040687 d loss 9.864314 w_loss 0.0030076883\n",
      "epoch: 4 *************************************************\n",
      "gen loss 0.0047299913 d loss 9.823099 w_loss 0.0041037756\n",
      "epoch: 5 *************************************************\n",
      "gen loss 0.0060255853 d loss 9.740541 w_loss 0.0058216257\n",
      "epoch: 6 *************************************************\n",
      "gen loss 0.007647969 d loss 9.670988 w_loss 0.0073640244\n",
      "epoch: 7 *************************************************\n",
      "gen loss 0.010108952 d loss 9.508235 w_loss 0.009057431\n",
      "epoch: 8 *************************************************\n",
      "gen loss 0.011747823 d loss 9.424486 w_loss 0.0132582635\n",
      "epoch: 9 *************************************************\n",
      "gen loss 0.014804979 d loss 9.233708 w_loss 0.01729004\n",
      "epoch: 10 *************************************************\n",
      "gen loss 0.015818799 d loss 9.091016 w_loss 0.02111205\n",
      "epoch: 11 *************************************************\n",
      "gen loss 0.018488916 d loss 8.883389 w_loss 0.023414794\n",
      "epoch: 12 *************************************************\n",
      "gen loss 0.020306852 d loss 8.629396 w_loss 0.03098574\n",
      "epoch: 13 *************************************************\n",
      "gen loss 0.022511553 d loss 8.411839 w_loss 0.03533431\n",
      "epoch: 14 *************************************************\n",
      "gen loss 0.024704784 d loss 8.044627 w_loss 0.041328385\n",
      "epoch: 15 *************************************************\n",
      "gen loss 0.026110146 d loss 7.7718506 w_loss 0.051791985\n",
      "epoch: 16 *************************************************\n",
      "gen loss 0.02808997 d loss 7.2266145 w_loss 0.057995122\n",
      "epoch: 17 *************************************************\n",
      "gen loss 0.029127885 d loss 6.870372 w_loss 0.07573892\n",
      "epoch: 18 *************************************************\n",
      "gen loss 0.027533462 d loss 6.5705605 w_loss 0.08395911\n",
      "epoch: 19 *************************************************\n",
      "gen loss 0.02472829 d loss 6.1755915 w_loss 0.087652765\n",
      "ending training\n",
      "Shape of the output of the GAN (10, 25)\n",
      "Reshaping the GAN output (in order to apply inverse scaling) (50, 5)\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "predictions, generator = learn_hypersurface_from_POD_coeffs(input_to_GAN, training_data, nsteps, nPOD, ndims, lmbda, n_critic, batch_size, batches, ndims_latent_input)\n",
    "t_train = time.time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dd1e4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time 10.99291181564331\n",
      "shape of predictions before writing to file (5, 50)\n",
      "Time taken to learn the hypersurface:  10.99291181564331\n",
      "optimisation part...\n",
      "nsteps 5 nLatent 5\n",
      "training_data (200, 25)\n",
      "inn (1, 20)\n",
      "*** predicting time step  0\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "WARNING:tensorflow:AutoGraph could not transform <function opt_latent_var at 0x7f71150523a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function opt_latent_var at 0x7f71150523a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n",
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n",
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "*** predicting time step  1\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n",
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n",
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "*** predicting time step  2\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n",
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n",
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "*** predicting time step  3\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n",
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n",
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "*** predicting time step  4\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n",
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n",
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "*** predicting time step  5\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n",
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n",
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "*** predicting time step  6\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n",
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "*** predicting time step  7\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n",
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n",
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "*** predicting time step  8\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n",
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n",
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "*** predicting time step  9\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n",
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n",
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "*** predicting time step  10\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n",
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n",
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "*** predicting time step  11\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n",
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n",
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "*** predicting time step  12\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n",
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n",
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "*** predicting time step  13\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n",
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "*** predicting time step  14\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n",
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n",
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "*** predicting time step  15\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n",
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n",
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "*** predicting time step  16\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n",
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n",
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "*** predicting time step  17\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n",
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n",
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "*** predicting time step  18\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n",
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n",
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "*** predicting time step  19\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n",
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n",
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "flds (24, 5)\n",
      "shape of predictions before writing to file (5, 24)\n",
      "python 3.8.10 (default, May 19 2021, 18:05:58) \n",
      "[GCC 7.3.0]\n",
      "numpy 1.20.2\n",
      "tf 2.4.1\n"
     ]
    }
   ],
   "source": [
    "t_train = time.time() - t0\n",
    "print('training time', t_train)\n",
    "\n",
    "# rescale \n",
    "predictions = scaling.inverse_transform(predictions).T\n",
    "print('shape of predictions before writing to file', predictions.shape)\n",
    "\n",
    "np.savetxt('prediction_from_GAN.csv', predictions, delimiter=',')  #save gan input if continuing training after job ends\n",
    "\n",
    "print('Time taken to learn the hypersurface: ', t_train)\n",
    "\n",
    "f = open('log.txt',\"a\")\n",
    "f.write('Time taken to train: %s \\n' % str(t_train)  )\n",
    "#    f.write( '%s ' % str(t_train) )\n",
    "f.close()\n",
    "\n",
    "# optimisation part -------------------------------------------------\n",
    "print('optimisation part...')\n",
    "# reproducibility\n",
    "np.random.seed(98)\n",
    "tf.random.set_seed(98)\n",
    "\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(5e-3)\n",
    "def mse_loss(inp, outp):\n",
    "    return mse(inp, outp)\n",
    "\n",
    "nLatent = ndims_latent_input\n",
    "print('nsteps',nsteps,'nLatent',nLatent)\n",
    "\n",
    "@tf.function\n",
    "def opt_latent_var(latent_var, output):   #main input optimization loop, optimizes input1 (a tf.variable) based on mse between known real output and generator output\n",
    "    #inpt = input1\n",
    "    #rp = real_outpt\n",
    "    \n",
    "    #print('******** in opt_latent_var', latent_var.shape, output.shape) \n",
    "    #tf.print('******** input1 before optimisation',input1.read_value())\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(latent_var)\n",
    "        r = generator(latent_var, training=False)\n",
    "        loss1 = mse_loss(output, r[:,:nLatent*(nsteps - 1)])\n",
    "\n",
    "    gradients = tape.gradient(loss1, latent_var)\n",
    "    optimizer.apply_gradients(zip([gradients], [latent_var]))\n",
    "\n",
    "    norm_latent_vars = tf.norm(latent_var)\n",
    "\n",
    "    ## clipping to within 2.3 is equivalent to 98%\n",
    "    #if norm_latent_vars > 2.3:\n",
    "    #    latent_var = 2.3 / norm_latent_vars * latent_var \n",
    "    #    tf.print('clipping to ', tf.norm(latent_var)) \n",
    "    \n",
    "    #tf.print('******** input1 after optimisation',input1.read_value())\n",
    "    #tf.print('******** inpt after optimisation  ',inpt.read_value())\n",
    "    return loss1, norm_latent_vars\n",
    "\n",
    "\n",
    "\n",
    "def timestep_loop(real_outpt1, previous_latent_vars, attempts): #optimizes inputs - either new randonly initialised inputs, or inputs from previous timestep\n",
    "\n",
    "    inputs = []\n",
    "    losses = []\n",
    "\n",
    "    loss_list = []\n",
    "    norm_latent_vars_list = []\n",
    "\n",
    "    initial_latent_variables = previous_latent_vars.numpy()\n",
    "    \n",
    "    print('in train all but initial, type previous_latent_vars / op2', type(previous_latent_vars)) \n",
    "\n",
    "    for j in range(attempts):\n",
    "\n",
    "        ip = previous_latent_vars\n",
    "        \n",
    "        for epoch in range(nepochs_optimiser):\n",
    "         \n",
    "            if epoch%100 == 0:   \n",
    "                print('******** epoch', epoch)            \n",
    "            loss1, norm_latent_vars = opt_latent_var(ip, real_outpt1)\n",
    "\n",
    "            loss_list.append(loss1)\n",
    "            norm_latent_vars_list.append(norm_latent_vars)\n",
    "\n",
    "        r = generator(ip, training=False)  \n",
    "        loss = mse_loss(real_outpt1, r[:,:nLatent*(nsteps - 1)])\n",
    "\n",
    "        inputt = ip.numpy()\n",
    "        loss_input = loss.numpy()\n",
    "\n",
    "        #inputs.append(inputt)\n",
    "        #losses.append(loss_input)\n",
    "\n",
    "\n",
    "    #initial_inputs = np.array(inputs)\n",
    "    #loss_for_initial_inputs = np.array(losses)\n",
    "    #initial_inputs = inputt\n",
    "    #loss_for_initial_inputs = loss_input\n",
    "\n",
    "    #min_loss = np.argmin(loss_for_initial_inputs)\n",
    "    #best_ipt = initial_inputs[min_loss]\n",
    "\n",
    "    return ip, loss_list, inputt, initial_latent_variables, norm_latent_vars_list #best_ipt\n",
    "\n",
    "\n",
    "def timesteps(initial, inn, iterations):  #timestep prediction\n",
    "    next_input1 = tf.convert_to_tensor(inn)\n",
    "    flds = tf.convert_to_tensor(initial)\n",
    "\n",
    "    losses_from_opt = []\n",
    "    norm_latent_vars_all_time_list = []\n",
    "    converged_inputs = np.zeros((iterations, 5))\n",
    "    initial_latent =  np.zeros((iterations, 5))\n",
    "  \n",
    "    ip1 = tf.zeros([1, nLatent]) #tf.random.normal([1, nLatent])\n",
    "    current = tf.Variable(ip1)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        print ('*** predicting time step ',i)\n",
    "        \n",
    "        # attempts=1 hard-wired\n",
    "        updated, loss_opt, converged_inputs[i,:], initial_latent[i,:], norm_latent_vars_list = timestep_loop(next_input1, current, 1) \n",
    "        current = updated\n",
    "\n",
    "        losses_from_opt.append(loss_opt)\n",
    "        norm_latent_vars_all_time_list.append(norm_latent_vars_list)\n",
    "        #print('norm_latent_vars_list:', len(norm_latent_vars_list), type(norm_latent_vars_list))\n",
    "\n",
    "        prediction = generator(updated, training=False)\n",
    "        #print('*** evaluate the generator with op2', prediction.numpy())\n",
    "        next_input = prediction[:,nLatent:] #last 4 images become next first 4 images\n",
    "        \n",
    "        new_result = prediction[:,nLatent*(nsteps - 1):]    #last image out of 5 is added to list of compressed vars\n",
    "        flds = tf.concat([flds, new_result], 0)\n",
    "\n",
    "        next_input1 = next_input.numpy()\n",
    "\n",
    "    #print('types loss_opt and norm_latent_vars', type(losses_from_opt), type(norm_latent_vars_all_time_list))\n",
    "\n",
    "    #np.savetxt('final_5_time_levels.csv', r_values, delimiter=',')\n",
    "    np.savetxt('optimised_losses.csv', losses_from_opt, delimiter=',')\n",
    "    np.savetxt('converged_z_values.csv', converged_inputs, delimiter=',')\n",
    "    np.savetxt('initial_z_values.csv', initial_latent, delimiter=',')\n",
    "    np.savetxt('norm_latent_vars.csv',norm_latent_vars_all_time_list,delimiter=',')\n",
    "\n",
    "    return flds\n",
    "\n",
    "####################################################################\n",
    "\n",
    "t0 = time.time()\n",
    "print('training_data', training_data.shape)\n",
    "start_from = 100\n",
    "inn = training_data[start_from,:(nsteps-1)*nPOD].reshape(1, (nsteps - 1) * nLatent)\n",
    "print('inn',inn.shape)\n",
    "npredictions = 20\n",
    "#nLatent = 5 #latent_input_size = 5\n",
    "nepochs_optimiser = 5000\n",
    "initial_comp = training_data[start_from,:(nsteps-1)*nPOD].reshape((nsteps - 1), nLatent)\n",
    "flds = timesteps(initial_comp, inn, npredictions)\n",
    "print('flds',flds.shape)\n",
    "\n",
    "# rescale \n",
    "flds = scaling.inverse_transform(flds).T\n",
    "print('shape of predictions before writing to file', flds.shape)\n",
    "\n",
    "np.savetxt('optimised_prediction_from_GAN.csv', initial_comp, inn, npredictions, delimiter=',')  #save gan input if continuing training after job ends\n",
    "\n",
    "t_optimise = time.time() - t0\n",
    "\n",
    "f = open('log.txt',\"a\")\n",
    "f.write('Time taken to optimise: %s \\n' % str(t_optimise)  )\n",
    "f.close()\n",
    "\n",
    "import sys\n",
    "print('python',sys.version)\n",
    "print('numpy', np.__version__)\n",
    "print('tf', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d0c6fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.48437694e+00  2.48444370e+00  2.48108772e+00  2.47933176e+00\n",
      "   2.47157381e+00  2.44318830e+00  2.43999882e+00  2.44120611e+00\n",
      "   2.44094482e+00  2.44102235e+00  2.44114754e+00  2.44105810e+00\n",
      "   2.44140379e+00  2.44470077e+00  2.44422283e+00  2.44438931e+00\n",
      "   2.44434238e+00  2.44434047e+00  2.44434967e+00  2.44434216e+00\n",
      "   2.44434313e+00  2.44432341e+00  2.44434021e+00  2.44434429e+00]\n",
      " [-4.79158543e-02  2.34611976e-01  4.19823169e-01  4.38214978e-01\n",
      "   1.28019132e-01 -7.32401690e-02 -1.10376114e-01 -8.24833627e-02\n",
      "  -8.86116108e-02 -8.77867357e-02 -8.74045890e-02 -8.92571116e-02\n",
      "  -8.76765416e-02 -5.35871252e-02 -6.16143742e-02 -5.97064979e-02\n",
      "  -6.01815672e-02 -6.02717543e-02 -6.01607699e-02 -6.02321657e-02\n",
      "  -6.02344015e-02 -6.04288724e-02 -6.02915519e-02 -6.02142999e-02]\n",
      " [-3.99152924e-01 -3.49496850e-01 -1.58300247e-01  9.86271254e-02\n",
      "  -7.25238469e-02  8.83169115e-03  1.90335943e-02  2.14992126e-02\n",
      "   2.08961215e-02  2.03950305e-02  2.06031760e-02  1.98230314e-02\n",
      "   1.95725239e-02  1.07291702e-03 -3.39317613e-03 -5.66622085e-03\n",
      "  -5.88084283e-03 -5.95150291e-03 -5.96183167e-03 -5.98349808e-03\n",
      "  -5.98339384e-03 -6.09834469e-03 -6.03132306e-03 -6.01241479e-03]\n",
      " [ 2.89000931e-02  2.34240339e-02 -1.30556992e-02 -3.14353737e-02\n",
      "  -6.57970010e-02 -1.71230287e-01 -1.57075032e-01 -1.75990746e-01\n",
      "  -1.73736680e-01 -1.74851775e-01 -1.73311449e-01 -1.74090060e-01\n",
      "  -1.72288289e-01 -1.66540037e-01 -1.68640632e-01 -1.69174478e-01\n",
      "  -1.69621727e-01 -1.69592250e-01 -1.69558157e-01 -1.69627479e-01\n",
      "  -1.69606409e-01 -1.69815761e-01 -1.69621995e-01 -1.69626376e-01]\n",
      " [ 6.97156250e-02  4.18793639e-02 -5.47836293e-02 -8.39272174e-02\n",
      "   2.59978641e-02 -1.50529215e-02 -6.22297708e-03 -1.32576375e-02\n",
      "  -1.21077032e-02 -1.23129373e-02 -1.20606998e-02 -1.18932333e-02\n",
      "  -1.15922821e-02 -1.14233845e-02 -1.00050017e-02 -9.89403147e-03\n",
      "  -9.88876489e-03 -9.85730419e-03 -9.86142765e-03 -9.86321003e-03\n",
      "  -9.85846102e-03 -9.85646270e-03 -9.84520972e-03 -9.85932993e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(flds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8520c02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
