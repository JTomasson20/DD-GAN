{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab4d64f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import sklearn.utils\n",
    "import sklearn.preprocessing\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "seed = 143\n",
    "\n",
    "initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05,\n",
    "                                                 seed=seed)\n",
    "\n",
    "def make_generator(nsteps):\n",
    "    generator = tf.keras.Sequential()\n",
    "    generator.add(tf.keras.layers.Dense(5, input_shape=(5,), activation='relu', kernel_initializer=initializer)) # 5\n",
    "    generator.add(tf.keras.layers.BatchNormalization())\n",
    "    generator.add(tf.keras.layers.Dense(10, activation='relu', kernel_initializer=initializer))             # 10\n",
    "    generator.add(tf.keras.layers.BatchNormalization())\n",
    "    generator.add(tf.keras.layers.Dense((5*nsteps), activation='relu', kernel_initializer=initializer))          # 25\n",
    "    generator.add(tf.keras.layers.BatchNormalization())\n",
    "    generator.add(tf.keras.layers.Dense((5*nsteps), activation='tanh', kernel_initializer=initializer))          # 25\n",
    "\n",
    "    return generator\n",
    "\n",
    "\n",
    "def make_critic(nsteps):\n",
    "    discriminator = tf.keras.Sequential()\n",
    "    discriminator.add(tf.keras.layers.Dense(5*nsteps, input_shape=(5*nsteps,), kernel_initializer=initializer))\n",
    "    discriminator.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "    discriminator.add(tf.keras.layers.Dropout(0.3))\n",
    "    discriminator.add(tf.keras.layers.Dense(10, kernel_initializer=initializer))\n",
    "    discriminator.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "    discriminator.add(tf.keras.layers.Dense(5, kernel_initializer=initializer))\n",
    "    discriminator.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "    discriminator.add(tf.keras.layers.Dropout(0.3))\n",
    "    discriminator.add(tf.keras.layers.Flatten())\n",
    "    discriminator.add(tf.keras.layers.Dense(1, kernel_initializer=initializer))\n",
    "\n",
    "\n",
    "    return discriminator\n",
    "\n",
    "\n",
    "def discriminator_loss(d_real, d_fake):\n",
    "    d_loss = tf.reduce_mean(d_fake) - tf.reduce_mean(d_real)\n",
    "    return d_loss\n",
    "\n",
    "def generator_loss(d_fake):\n",
    "    g_loss = -tf.reduce_mean(d_fake)\n",
    "    return g_loss\n",
    "\n",
    "@tf.function\n",
    "def train_step(noise, real, lmbda, n_critic, g, c, g_opt, c_opt, g1_loss, d1_loss, w_loss):   \n",
    "    batch_size = len(real)\n",
    "\n",
    "    ##### train critic ######\n",
    "    \n",
    "    #print('inside train step') \n",
    "\n",
    "    for i in range(n_critic):    \n",
    "        with tf.GradientTape() as t:\n",
    "            with tf.GradientTape() as t1:\n",
    "                fake = g(noise, training=True) # training=False?\n",
    "                # tf.print(fake)\n",
    "                epsilon = tf.random.uniform(shape=[batch_size, 1], minval=0., maxval=1.)\n",
    "                #print ('types', real.dtype, epsilon.dtype, fake.dtype)                \n",
    "                interpolated = real + epsilon * (fake - real) \n",
    "                t1.watch(interpolated)\n",
    "                c_inter = c(interpolated, training=True)  \n",
    "                d_real = c(real, training=True)\n",
    "                d_fake = c(fake, training=True)\n",
    "                d_loss = discriminator_loss(d_real, d_fake)   #initial c loss\n",
    "                \n",
    "                #print('d loop', d_real.shape,  d_fake.shape,  d_loss.shape, c_inter.shape) # batch_size by 1 except for d_loss\n",
    "                #print('d loop',  interpolated.shape, inpt.shape, real.shape, fake.shape) # \n",
    "                \n",
    "                \n",
    "                #print('discriminator loop',i ,'---------------------------------------------')    \n",
    "                #print('min and max values of fake',np.min(fake.numpy()),np.max(fake.numpy()))\n",
    "                #print('min and max values of real',np.min(real),np.max(real) )\n",
    "                #print('min and max values of d_fake',np.min(d_fake.numpy()),np.max(d_fake.numpy()))\n",
    "                #print('min and max values of d_real',np.min(d_real.numpy()),np.max(d_real.numpy()))\n",
    "                \n",
    "            grad_interpolated = t1.gradient(c_inter, interpolated)\n",
    "            \n",
    "            #print('interpolated      ', interpolated.numpy())\n",
    "            #print('c_inter           ', c_inter.numpy())\n",
    "            #print('grad_interpolated', grad_interpolated.numpy())\n",
    "            \n",
    "            #print('grad_interpolated itself ', grad_interpolated.numpy().shape) # batch_size by 25\n",
    "            #print('grad_interpolated square ', tf.square(grad_interpolated).numpy().shape) # batch_size by 25\n",
    "            #print('grad_interpolated red sum', tf.reduce_sum(tf.square(grad_interpolated), axis=[1]).numpy().shape) # batch_size by 1\n",
    "            #print('grad_interpolated sqrt', tf.sqrt(tf.reduce_sum(tf.square(grad_interpolated), axis=[1])).numpy().shape) # batch_size by 1            \n",
    "            \n",
    "                     #tf.sqrt(tf.reduce_sum(tf.square(x)) + 1.0e-12)\n",
    "            slopes = tf.sqrt(tf.reduce_sum(tf.square(grad_interpolated) + 1e-12, axis=[1])) # \n",
    "            \n",
    "            gradient_penalty = tf.reduce_mean((slopes - 1.) ** 2)\n",
    "            #print('slopes, grad penalty', slopes.numpy(), gradient_penalty.numpy())\n",
    "\n",
    "            new_d_loss = d_loss + (lmbda*gradient_penalty)  #new c loss\n",
    "            #print('d_loss and new_d_loss',d_loss.numpy(), new_d_loss.numpy()) \n",
    "        \n",
    "        c_grad = t.gradient(new_d_loss, c.trainable_variables)\n",
    "        #print('length and type c_grad', len(c_grad), type(c_grad))  # length of c_grad was 8? type list\n",
    "        #print('length and type c_grad[0]', (c_grad[0].shape), type(c_grad[0]))  # length of c_grad was 8? type list\n",
    "        #print('values of cgrad',c_grad)\n",
    "        #print('c.trainable_variables', c.trainable_variables)\n",
    "        c_opt.apply_gradients(zip(c_grad, c.trainable_variables))\n",
    "\n",
    "\n",
    "    ##### train generator #####\n",
    "\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        fake_images = g(noise, training=True)\n",
    "        d_fake = c(fake_images, training=True) # training=False?\n",
    "        g_loss = generator_loss(d_fake)\n",
    "\n",
    "    gen_grads = gen_tape.gradient(g_loss, g.trainable_variables)\n",
    "    g_opt.apply_gradients(zip(gen_grads, g.trainable_variables))\n",
    "    \n",
    "    \n",
    "    #print('g.trainable_variables')\n",
    "    #for v in g.trainable_variables:\n",
    "    #  print(v.name)\n",
    "    #print('c.trainable_variables')\n",
    "    #for v in c.trainable_variables:\n",
    "    #  print(v.name)\n",
    "\n",
    "    ### for tensorboard\n",
    "    g1_loss(g_loss)\n",
    "    d1_loss(new_d_loss)\n",
    "    w_loss((-1)*(d_loss))  #wasserstein distance\n",
    "\n",
    "\n",
    "    return \n",
    "\n",
    "def train(nsteps,ndims,lmbda,n_critic,batch_size,batches,training_data,input_to_GAN, epochs, g, c, g_opt, c_opt, g1_loss, d1_loss, w_loss, g1_summary_writer, d1_summary_writer, w_summary_writer):\n",
    "\n",
    "    losses = np.zeros((epochs,4))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        noise = input_to_GAN\n",
    "        real_data = training_data #X1.astype('int')\n",
    "\n",
    "        # uncommenting this line means that the noise is not paired with the outputs (probably desirable)\n",
    "        #noise = np.random.normal(size=[noise.shape[0],noise.shape[1]])\n",
    " \n",
    "        real_data, noise = sklearn.utils.shuffle(real_data, noise) #shuffle each epoch\n",
    "        #print ('shuffled l_input1 and xn1_comp')\n",
    "        \n",
    "        xx1 = real_data.reshape(batches, batch_size, ndims*nsteps)\n",
    "        inpt1 = noise.reshape(batches, batch_size, ndims)\n",
    "        #print ('data arranged in batches')\n",
    "        \n",
    "        #print(g.layers[0].weights)\n",
    "\n",
    "        for i in range(len(xx1)):\n",
    "            #print('calling train_step', i ,'of',len(xx1), '-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*')  \n",
    "            train_step(inpt1[i], xx1[i], lmbda, n_critic, g, c, g_opt, c_opt, g1_loss, d1_loss, w_loss)\n",
    "            #print('back from train_step')  \n",
    "\n",
    "        print('epoch:', epoch, '*************************************************')    \n",
    "        print('gen loss', g1_loss.result().numpy(), 'd loss', d1_loss.result().numpy(), 'w_loss' , w_loss.result().numpy())\n",
    "\n",
    "        losses[epoch,:] = [ epoch+1, g1_loss.result().numpy() ,   d1_loss.result().numpy(),  w_loss.result().numpy()]\n",
    "\n",
    "        with g1_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', g1_loss.result(), step=epoch)\n",
    "\n",
    "        with d1_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', d1_loss.result(), step=epoch)\n",
    "\n",
    "        with w_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', w_loss.result(), step=epoch)\n",
    "            \n",
    "        #print('reset states')\n",
    "        g1_loss.reset_states()\n",
    "        d1_loss.reset_states()\n",
    "        w_loss.reset_states()\n",
    "\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "        #if epoch < 100 or (epoch + 1) % 100 == 0 :\n",
    "                       \n",
    "            saved_g1_dir = './saved_g_' + str(epoch + 1)\n",
    "            saved_d1_dir = './saved_c_' + str(epoch + 1)\n",
    "            tf.keras.models.save_model(g, saved_g1_dir)\n",
    "            tf.keras.models.save_model(c, saved_d1_dir)\n",
    "\n",
    "    np.savetxt('losses.csv', losses, delimiter=',')\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "def learn_hypersurface_from_POD_coeffs(input_to_GAN, training_data, nsteps, nPOD, ndims, lmbda, n_critic, batch_size, batches, ndims_latent_input):\n",
    "    # nPOD not needed\n",
    "\n",
    "    try:\n",
    "      print('looking for previous saved models')\n",
    "      saved_g1_dir = './saved_g_' + str(model_number)\n",
    "      g = tf.keras.models.load_model(saved_g1_dir)\n",
    "\n",
    "      saved_d1_dir = './saved_c_' + str(model_number)\n",
    "      c = tf.keras.models.load_model(saved_d1_dir)\n",
    "\n",
    "\n",
    "    except:\n",
    "      print('making new generator and critic')\n",
    "      g = make_generator(nsteps)\n",
    "      c = make_critic(nsteps)\n",
    "\n",
    "\n",
    "    g_opt = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0, beta_2=0.9)\n",
    "    c_opt = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0, beta_2=0.9)\n",
    "\n",
    "    g1_loss = tf.keras.metrics.Mean('g1_loss', dtype=tf.float32)\n",
    "    d1_loss = tf.keras.metrics.Mean('d1_loss', dtype=tf.float32)\n",
    "    w_loss = tf.keras.metrics.Mean('w_loss', dtype=tf.float32)\n",
    "\n",
    "    # logs to follow losses on tensorboard\n",
    "    print('initialising logs for tensorboard')\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    g1_log_dir = './logs/gradient_tape/' + current_time + '/g'\n",
    "    d1_log_dir = './logs/gradient_tape/' + current_time + '/d'\n",
    "    w_log_dir = './logs/gradient_tape/' + current_time + '/w'\n",
    "\n",
    "    g1_summary_writer = tf.summary.create_file_writer(g1_log_dir)\n",
    "    d1_summary_writer = tf.summary.create_file_writer(d1_log_dir)\n",
    "    w_summary_writer = tf.summary.create_file_writer(w_log_dir)\n",
    "\n",
    "\n",
    "    print('beginning training')\n",
    "    epochs = 20\n",
    "    generator = train(nsteps,ndims,lmbda,n_critic,batch_size,batches,training_data,input_to_GAN, epochs, g, c, g_opt, c_opt, g1_loss, d1_loss, w_loss, g1_summary_writer, d1_summary_writer, w_summary_writer)\n",
    "    print('ending training')\n",
    "\n",
    "\n",
    "    # generate some random inputs and put through generator\n",
    "    number_test_examples = 10\n",
    "    test_input = tf.random.normal([number_test_examples, ndims_latent_input])\n",
    "    predictions = generator(test_input, training=False)\n",
    "    #predictions = generator.predict(test_input) # number_test_examples by ndims_latent_input\n",
    "\n",
    "    predictions_np = predictions.numpy() # nExamples by nPOD*nsteps\n",
    "    #tf.compat.v1.InteractiveSession()\n",
    "    #predictions_np = predictions.numpy().\n",
    "    print('Shape of the output of the GAN', predictions_np.shape)\n",
    "    predictions_np = predictions_np.reshape(number_test_examples*nsteps, nPOD)\n",
    "    print('Reshaping the GAN output (in order to apply inverse scaling)', predictions_np.shape)\n",
    "\n",
    "    return predictions_np, generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1b05287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in the POD coeffs.\n",
      "type and shape (nPOD by nTrain) of POD coeffs from csv file <class 'numpy.ndarray'> (5, 204) float32\n",
      "shape csv data for the scaling (204, 5)\n",
      "min and max of col,  0  of csv_data: -1.0 1.0\n",
      "min and max of col,  1  of csv_data: -1.0 1.0000001\n",
      "min and max of col,  2  of csv_data: -1.0 1.0\n",
      "min and max of col,  3  of csv_data: -1.0 1.0\n",
      "min and max of col,  4  of csv_data: -1.0 1.0\n",
      "Shape of training data for the GAN (200, 25) float32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "# reproducibility\n",
    "import random\n",
    "\n",
    "np.random.seed(143)\n",
    "random.seed(143)\n",
    "tf.random.set_seed(143)\n",
    "\n",
    "# read in data, reshape and normalise\n",
    "lmbda = 10\n",
    "n_critic = 5\n",
    "\n",
    "batch_size = 20 # 32  \n",
    "batches = 10    # 900 \n",
    "\n",
    "ndims_latent_input = 5 # 128 # latent variables for GAN\n",
    "\n",
    "# data settings\n",
    "nsteps = 5  #number of consecutive timesteps in gan\n",
    "ndims = 5 # == nPOD # 128 # reduced variables i.e. POD coefficients or AE latent variables\n",
    "\n",
    "# reading in the data\n",
    "print('Reading in the POD coeffs.')\n",
    "#csv_data = np.loadtxt('/kaggle/input/fpc-204examples-5pod-without-ic/POD_coeffs_1_204.csv', delimiter=',')\n",
    "csv_data = np.loadtxt('../data/processed/POD_coeffs_1_204_orig.csv', delimiter=',')\n",
    "csv_data = np.float32(csv_data)\n",
    "print('type and shape (nPOD by nTrain) of POD coeffs from csv file', type(csv_data), csv_data.shape, csv_data.dtype)\n",
    "\n",
    "nTrain = csv_data.shape[1]\n",
    "nPOD = csv_data.shape[0]\n",
    "\n",
    "csv_data = csv_data.T # nTrain by nPOD\n",
    "\n",
    "# scaling the POD coeffs\n",
    "scaling = sklearn.preprocessing.MinMaxScaler(feature_range=[-1,1])\n",
    "print('shape csv data for the scaling', csv_data.shape) \n",
    "csv_data = scaling.fit_transform(csv_data)\n",
    "\n",
    "# check that the columns are scaled between min and max values (-1,1)\n",
    "for icol in range(csv_data.shape[1]):\n",
    "    print('min and max of col, ', icol ,' of csv_data:', np.min(csv_data[:,icol]), np.max(csv_data[:,icol]) )\n",
    "\n",
    "# create nsteps time levels for the training_data for the GAN\n",
    "t_begin = 0\n",
    "t_end = nTrain - nsteps + 1\n",
    "training_data = np.zeros((t_end,nPOD*nsteps),dtype=np.float32) # nTrain by nsteps*nPOD # 'float32' or np.float32\n",
    "\n",
    "for step in range(nsteps):\n",
    "    #print ('training data - cols',step*nPOD,'to',(step+1)*nPOD )\n",
    "    #print ('csv data - rows', t_begin+step ,'to', t_end+step )\n",
    "    training_data[:,step*nPOD:(step+1)*nPOD] = csv_data[t_begin+step : t_end+step,:]\n",
    "\n",
    "print('Shape of training data for the GAN', training_data.shape, training_data.dtype)\n",
    "\n",
    "# GAN input\n",
    "try:\n",
    "    input_to_GAN = np.load('input_to_GAN.npy')\n",
    "except:\n",
    "    input_to_GAN = tf.random.normal([training_data.shape[0], ndims_latent_input])\n",
    "    input_to_GAN = input_to_GAN.numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeebc74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking for previous saved models\n",
      "making new generator and critic\n",
      "initialising logs for tensorboard\n",
      "beginning training\n",
      "epoch: 0 *************************************************\n",
      "gen loss 0.00044128043 d loss 9.975678 w_loss 0.00051889766\n",
      "epoch: 1 *************************************************\n",
      "gen loss 0.0014283005 d loss 9.944265 w_loss 0.0013156604\n",
      "epoch: 2 *************************************************\n",
      "gen loss 0.0023684455 d loss 9.915084 w_loss 0.0020202943\n",
      "epoch: 3 *************************************************\n",
      "gen loss 0.0036040687 d loss 9.864314 w_loss 0.0030076883\n",
      "epoch: 4 *************************************************\n",
      "gen loss 0.0047299913 d loss 9.823099 w_loss 0.0041037756\n",
      "epoch: 5 *************************************************\n",
      "gen loss 0.0060255853 d loss 9.740541 w_loss 0.0058216257\n",
      "epoch: 6 *************************************************\n",
      "gen loss 0.007647969 d loss 9.670988 w_loss 0.0073640244\n",
      "epoch: 7 *************************************************\n",
      "gen loss 0.010108952 d loss 9.508235 w_loss 0.009057431\n",
      "epoch: 8 *************************************************\n",
      "gen loss 0.011747823 d loss 9.424486 w_loss 0.0132582635\n",
      "epoch: 9 *************************************************\n",
      "gen loss 0.014804979 d loss 9.233708 w_loss 0.01729004\n",
      "epoch: 10 *************************************************\n",
      "gen loss 0.015818799 d loss 9.091016 w_loss 0.02111205\n",
      "epoch: 11 *************************************************\n",
      "gen loss 0.018488916 d loss 8.883389 w_loss 0.023414794\n",
      "epoch: 12 *************************************************\n",
      "gen loss 0.020306852 d loss 8.629396 w_loss 0.03098574\n",
      "epoch: 13 *************************************************\n",
      "gen loss 0.022511553 d loss 8.411839 w_loss 0.03533431\n",
      "epoch: 14 *************************************************\n",
      "gen loss 0.024704784 d loss 8.044627 w_loss 0.041328385\n",
      "epoch: 15 *************************************************\n",
      "gen loss 0.026110146 d loss 7.7718506 w_loss 0.051791985\n",
      "epoch: 16 *************************************************\n",
      "gen loss 0.02808997 d loss 7.2266145 w_loss 0.057995122\n",
      "epoch: 17 *************************************************\n",
      "gen loss 0.029127885 d loss 6.870372 w_loss 0.07573892\n",
      "epoch: 18 *************************************************\n",
      "gen loss 0.027533462 d loss 6.5705605 w_loss 0.08395911\n",
      "epoch: 19 *************************************************\n",
      "gen loss 0.02472829 d loss 6.1755915 w_loss 0.087652765\n",
      "ending training\n",
      "Shape of the output of the GAN (10, 25)\n",
      "Reshaping the GAN output (in order to apply inverse scaling) (50, 5)\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "predictions, generator = learn_hypersurface_from_POD_coeffs(input_to_GAN, training_data, nsteps, nPOD, ndims, lmbda, n_critic, batch_size, batches, ndims_latent_input)\n",
    "t_train = time.time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ceb6295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time 15.629953861236572\n",
      "shape of predictions before writing to file (5, 50)\n",
      "Time taken to learn the hypersurface:  15.629953861236572\n",
      "optimisation part...\n",
      "nsteps 5 nLatent 5\n",
      "training_data (200, 25)\n",
      "inn (1, 20)\n",
      "*** predicting time step  0\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "WARNING:tensorflow:AutoGraph could not transform <function opt_latent_var at 0x7f6892c0b310> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function opt_latent_var at 0x7f6892c0b310> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n",
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n",
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "*** predicting time step  1\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n",
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n",
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "*** predicting time step  2\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n",
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n",
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "*** predicting time step  3\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n",
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n",
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "*** predicting time step  4\n",
      "in train all but initial, type previous_latent_vars / op2 <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "******** epoch 0\n",
      "******** epoch 100\n",
      "******** epoch 200\n",
      "******** epoch 300\n",
      "******** epoch 400\n",
      "******** epoch 500\n",
      "******** epoch 600\n",
      "******** epoch 700\n",
      "******** epoch 800\n",
      "******** epoch 900\n",
      "******** epoch 1000\n",
      "******** epoch 1100\n",
      "******** epoch 1200\n",
      "******** epoch 1300\n",
      "******** epoch 1400\n",
      "******** epoch 1500\n",
      "******** epoch 1600\n",
      "******** epoch 1700\n",
      "******** epoch 1800\n",
      "******** epoch 1900\n",
      "******** epoch 2000\n",
      "******** epoch 2100\n",
      "******** epoch 2200\n",
      "******** epoch 2300\n",
      "******** epoch 2400\n",
      "******** epoch 2500\n",
      "******** epoch 2600\n",
      "******** epoch 2700\n",
      "******** epoch 2800\n",
      "******** epoch 2900\n",
      "******** epoch 3000\n",
      "******** epoch 3100\n",
      "******** epoch 3200\n",
      "******** epoch 3300\n",
      "******** epoch 3400\n",
      "******** epoch 3500\n",
      "******** epoch 3600\n",
      "******** epoch 3700\n",
      "******** epoch 3800\n",
      "******** epoch 3900\n",
      "******** epoch 4000\n",
      "******** epoch 4100\n",
      "******** epoch 4200\n",
      "******** epoch 4300\n",
      "******** epoch 4400\n",
      "******** epoch 4500\n",
      "******** epoch 4600\n",
      "******** epoch 4700\n",
      "******** epoch 4800\n",
      "******** epoch 4900\n",
      "flds (9, 5)\n",
      "shape of predictions before writing to file (5, 9)\n",
      "python 3.8.10 (default, May 19 2021, 18:05:58) \n",
      "[GCC 7.3.0]\n",
      "numpy 1.20.2\n",
      "tf 2.4.1\n"
     ]
    }
   ],
   "source": [
    "t_train = time.time() - t0\n",
    "print('training time', t_train)\n",
    "\n",
    "# rescale \n",
    "predictions = scaling.inverse_transform(predictions).T\n",
    "print('shape of predictions before writing to file', predictions.shape)\n",
    "\n",
    "np.savetxt('prediction_from_GAN.csv', predictions, delimiter=',')  #save gan input if continuing training after job ends\n",
    "\n",
    "print('Time taken to learn the hypersurface: ', t_train)\n",
    "\n",
    "f = open('log.txt',\"a\")\n",
    "f.write('Time taken to train: %s \\n' % str(t_train)  )\n",
    "#    f.write( '%s ' % str(t_train) )\n",
    "f.close()\n",
    "\n",
    "# optimisation part -------------------------------------------------\n",
    "print('optimisation part...')\n",
    "# reproducibility\n",
    "np.random.seed(98)\n",
    "tf.random.set_seed(98)\n",
    "\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(5e-3)\n",
    "def mse_loss(inp, outp):\n",
    "    return mse(inp, outp)\n",
    "\n",
    "nLatent = ndims_latent_input\n",
    "print('nsteps',nsteps,'nLatent',nLatent)\n",
    "\n",
    "@tf.function\n",
    "def opt_latent_var(latent_var, output):   #main input optimization loop, optimizes input1 (a tf.variable) based on mse between known real output and generator output\n",
    "    #inpt = input1\n",
    "    #rp = real_outpt\n",
    "    \n",
    "    #print('******** in opt_latent_var', latent_var.shape, output.shape) \n",
    "    #tf.print('******** input1 before optimisation',input1.read_value())\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(latent_var)\n",
    "        r = generator(latent_var, training=False)  \n",
    "        loss1 = mse_loss(output, r[:,:nLatent*(nsteps - 1)])\n",
    "\n",
    "    gradients = tape.gradient(loss1, latent_var)\n",
    "    optimizer.apply_gradients(zip([gradients], [latent_var]))\n",
    "\n",
    "    norm_latent_vars = tf.norm(latent_var)\n",
    "\n",
    "    ## clipping to within 2.3 is equivalent to 98%\n",
    "    #if norm_latent_vars > 2.3:\n",
    "    #    latent_var = 2.3 / norm_latent_vars * latent_var \n",
    "    #    tf.print('clipping to ', tf.norm(latent_var)) \n",
    "    \n",
    "    #tf.print('******** input1 after optimisation',input1.read_value())\n",
    "    #tf.print('******** inpt after optimisation  ',inpt.read_value())\n",
    "    return loss1, norm_latent_vars\n",
    "\n",
    "\n",
    "\n",
    "def timestep_loop(real_outpt1, previous_latent_vars, attempts): #optimizes inputs - either new randonly initialised inputs, or inputs from previous timestep\n",
    "\n",
    "    inputs = []\n",
    "    losses = []\n",
    "\n",
    "    loss_list = []\n",
    "    norm_latent_vars_list = []\n",
    "\n",
    "    initial_latent_variables = previous_latent_vars.numpy()\n",
    "    \n",
    "    print('in train all but initial, type previous_latent_vars / op2', type(previous_latent_vars)) \n",
    "\n",
    "    for j in range(attempts):\n",
    "\n",
    "        ip = previous_latent_vars\n",
    "        \n",
    "        for epoch in range(nepochs_optimiser):\n",
    "         \n",
    "            if epoch%100 == 0:   \n",
    "                print('******** epoch', epoch)            \n",
    "            loss1, norm_latent_vars = opt_latent_var(ip, real_outpt1)\n",
    "\n",
    "            loss_list.append(loss1)\n",
    "            norm_latent_vars_list.append(norm_latent_vars)\n",
    "\n",
    "        r = generator(ip, training=False)  \n",
    "        loss = mse_loss(real_outpt1, r[:,:nLatent*(nsteps - 1)])\n",
    "\n",
    "        inputt = ip.numpy()\n",
    "        loss_input = loss.numpy()\n",
    "\n",
    "        #inputs.append(inputt)\n",
    "        #losses.append(loss_input)\n",
    "\n",
    "\n",
    "    #initial_inputs = np.array(inputs)\n",
    "    #loss_for_initial_inputs = np.array(losses)\n",
    "    #initial_inputs = inputt\n",
    "    #loss_for_initial_inputs = loss_input\n",
    "\n",
    "    #min_loss = np.argmin(loss_for_initial_inputs)\n",
    "    #best_ipt = initial_inputs[min_loss]\n",
    "\n",
    "    return ip, loss_list, inputt, initial_latent_variables, norm_latent_vars_list #best_ipt\n",
    "\n",
    "\n",
    "def timesteps(initial, inn, iterations):  #timestep prediction\n",
    "    next_input1 = tf.convert_to_tensor(inn)\n",
    "    flds = tf.convert_to_tensor(initial)\n",
    "\n",
    "    losses_from_opt = []\n",
    "    norm_latent_vars_all_time_list = []\n",
    "    converged_inputs = np.zeros((iterations, 5))\n",
    "    initial_latent =  np.zeros((iterations, 5))\n",
    "  \n",
    "    ip1 = tf.zeros([1, nLatent]) #tf.random.normal([1, nLatent])\n",
    "    current = tf.Variable(ip1)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        print ('*** predicting time step ',i)\n",
    "        \n",
    "        # attempts=1 hard-wired\n",
    "        updated, loss_opt, converged_inputs[i,:], initial_latent[i,:], norm_latent_vars_list = timestep_loop(next_input1, current, 1) \n",
    "        current = updated\n",
    "\n",
    "        losses_from_opt.append(loss_opt)\n",
    "        norm_latent_vars_all_time_list.append(norm_latent_vars_list)\n",
    "        #print('norm_latent_vars_list:', len(norm_latent_vars_list), type(norm_latent_vars_list))\n",
    "\n",
    "        prediction = generator(updated, training=False)\n",
    "        #print('*** evaluate the generator with op2', prediction.numpy())\n",
    "        next_input = prediction[:,nLatent:] #last 4 images become next first 4 images\n",
    "        \n",
    "        new_result = prediction[:,nLatent*(nsteps - 1):]    #last image out of 5 is added to list of compressed vars\n",
    "        flds = tf.concat([flds, new_result], 0)\n",
    "\n",
    "        next_input1 = next_input.numpy()\n",
    "\n",
    "    #print('types loss_opt and norm_latent_vars', type(losses_from_opt), type(norm_latent_vars_all_time_list))\n",
    "\n",
    "    #np.savetxt('final_5_time_levels.csv', r_values, delimiter=',')\n",
    "    np.savetxt('optimised_losses.csv', losses_from_opt, delimiter=',')\n",
    "    np.savetxt('converged_z_values.csv', converged_inputs, delimiter=',')\n",
    "    np.savetxt('initial_z_values.csv', initial_latent, delimiter=',')\n",
    "    np.savetxt('norm_latent_vars.csv',norm_latent_vars_all_time_list,delimiter=',')\n",
    "\n",
    "    return flds\n",
    "\n",
    "####################################################################\n",
    "\n",
    "t0 = time.time()\n",
    "print('training_data', training_data.shape)\n",
    "start_from = 100\n",
    "inn = training_data[start_from,:(nsteps-1)*nPOD].reshape(1, (nsteps - 1) * nLatent)\n",
    "print('inn',inn.shape)\n",
    "npredictions = 20\n",
    "#nLatent = 5 #latent_input_size = 5\n",
    "nepochs_optimiser = 5000\n",
    "initial_comp = training_data[start_from,:(nsteps-1)*nPOD].reshape((nsteps - 1), nLatent)\n",
    "flds = timesteps(initial_comp, inn, npredictions)\n",
    "print('flds',flds.shape)\n",
    "\n",
    "# rescale \n",
    "flds = scaling.inverse_transform(flds).T\n",
    "print('shape of predictions before writing to file', flds.shape)\n",
    "\n",
    "np.savetxt('optimised_prediction_from_GAN.csv', flds, delimiter=',')  #save gan input if continuing training after job ends\n",
    "\n",
    "t_optimise = time.time() - t0\n",
    "\n",
    "f = open('log.txt',\"a\")\n",
    "f.write('Time taken to optimise: %s \\n' % str(t_optimise)  )\n",
    "f.close()\n",
    "\n",
    "import sys\n",
    "print('python',sys.version)\n",
    "print('numpy', np.__version__)\n",
    "print('tf', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2529873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.44688702e+00  2.43802261e+00  2.44731712e+00  2.46251202e+00\n",
      "   2.43493795e+00  2.46606612e+00  2.45823216e+00  2.47894692e+00\n",
      "   2.44551158e+00  2.46451092e+00  2.44705248e+00  2.47373819e+00\n",
      "   2.45815468e+00  2.44870877e+00  2.46743631e+00  2.46765709e+00\n",
      "   2.44965243e+00  2.45142102e+00  2.44660807e+00  2.45179462e+00\n",
      "   2.45011663e+00  2.46357393e+00  2.44808316e+00  2.44209743e+00\n",
      "   2.46775651e+00  2.45772958e+00  2.48550653e+00  2.44723320e+00\n",
      "   2.45054293e+00  2.47554326e+00  2.44133592e+00  2.44777131e+00\n",
      "   2.44264460e+00  2.44682574e+00  2.46051383e+00  2.44986176e+00\n",
      "   2.46077013e+00  2.44954586e+00  2.44029737e+00  2.46743703e+00\n",
      "   2.47582984e+00  2.46728063e+00  2.45347548e+00  2.45031524e+00\n",
      "   2.46763992e+00  2.45219660e+00  2.43831372e+00  2.44931006e+00\n",
      "   2.45966887e+00  2.43797827e+00]\n",
      " [-7.87817538e-02 -4.78071533e-02 -8.26811194e-02 -1.10708587e-01\n",
      "  -1.63466409e-01  3.32754590e-02  2.88317949e-01  1.67159475e-02\n",
      "   2.44626760e-01  9.66182947e-02  3.37659232e-02 -2.92063016e-03\n",
      "   1.38034075e-01  1.70435473e-01  2.11339369e-01  1.65995494e-01\n",
      "   6.69420278e-03 -7.81206042e-02 -9.20115486e-02  3.48279327e-02\n",
      "  -4.17542923e-03 -8.33775699e-02  5.65690063e-02  9.75236297e-02\n",
      "   9.06331986e-02  1.57647148e-01 -4.02548425e-02  2.24883601e-01\n",
      "   1.80284455e-01  2.34034166e-01  2.87694717e-03 -1.38082519e-01\n",
      "   2.63943281e-02 -3.67311314e-02  3.95900123e-02 -4.78490032e-02\n",
      "  -6.94798157e-02  2.37910412e-02  1.02149770e-01  7.09844902e-02\n",
      "   3.63767385e-01 -6.62621483e-02 -7.58620724e-02 -1.05775103e-01\n",
      "   4.79462482e-02 -5.40391989e-02  2.21193233e-03 -7.46170804e-02\n",
      "  -5.39571121e-02 -1.32238328e-01]\n",
      " [ 4.17275354e-02  5.94725572e-02  9.59650241e-03  1.05633400e-01\n",
      "   1.06526548e-02  1.09504074e-01 -5.40723689e-02  5.19833602e-02\n",
      "   7.09366053e-02  2.08085589e-02 -5.59181459e-02  3.99055034e-02\n",
      "   1.02744542e-01 -1.17086008e-01  3.82682867e-02 -7.38774985e-02\n",
      "  -1.07974395e-01 -1.14110358e-01 -9.34950560e-02 -1.34610891e-01\n",
      "   2.49125622e-02  1.04251560e-02  7.01989159e-02 -8.52730498e-02\n",
      "   4.08029482e-02 -1.40346959e-01  1.30203038e-01  1.50963247e-01\n",
      "  -2.09977165e-01  2.64782645e-02  7.49130696e-02 -1.64475590e-02\n",
      "   4.10367474e-02 -1.77593753e-02  8.08991343e-02  4.51946557e-02\n",
      "  -2.95706070e-03  7.11725503e-02 -5.35649098e-02  4.79124449e-02\n",
      "  -2.00194508e-01 -1.23710848e-01 -8.75715390e-02 -1.29927576e-01\n",
      "  -2.58183599e-01  5.13301492e-02  3.94389778e-02  1.84651539e-02\n",
      "   4.34233509e-02  2.14468874e-02]\n",
      " [-1.40619010e-01 -1.15511850e-01 -1.76997915e-01 -2.17006162e-01\n",
      "  -2.11192295e-01 -2.60142386e-01 -2.15597570e-01 -1.66397244e-01\n",
      "  -1.24012575e-01 -3.39031070e-02 -2.10394621e-01 -9.40802246e-02\n",
      "  -1.92473441e-01 -1.04753278e-01 -3.52986045e-02 -1.18180938e-01\n",
      "  -2.22747996e-01 -1.19456671e-01 -1.72547504e-01 -1.60274208e-01\n",
      "  -1.72251940e-01 -9.66471881e-02 -1.26604676e-01 -9.23618302e-02\n",
      "  -1.05735272e-01 -2.02386618e-01 -1.14931434e-01 -3.00456733e-01\n",
      "  -1.94424354e-02  2.19696034e-02 -1.11679956e-01 -1.40486568e-01\n",
      "  -1.45853072e-01 -1.56678334e-01 -1.52858943e-01 -1.66782111e-01\n",
      "  -8.61204043e-02 -9.47299376e-02 -9.86487716e-02 -1.21956989e-01\n",
      "  -1.44665331e-01 -3.22122753e-01 -1.20913751e-01 -1.26485616e-01\n",
      "  -2.47684717e-02 -1.58223346e-01 -9.65658426e-02 -1.63488537e-01\n",
      "  -1.78861469e-01 -1.67658672e-01]\n",
      " [ 3.05097979e-02  2.16223262e-02  1.17411120e-02  1.32782292e-02\n",
      "   1.68565908e-04 -6.49126619e-03  2.87745949e-02 -8.12135451e-03\n",
      "   2.98302025e-02 -3.35428282e-04 -2.76736300e-02 -1.70257483e-02\n",
      "   5.72434068e-02 -6.68323555e-05  4.49818671e-02  4.09913696e-02\n",
      "   1.24684433e-02 -3.59164290e-02  1.61839351e-02 -4.21092147e-04\n",
      "  -1.84659380e-02 -1.96096636e-02  5.46430275e-02 -1.65237859e-02\n",
      "  -2.32883822e-03 -4.43861708e-02 -2.83298530e-02  6.78822324e-02\n",
      "  -1.11709144e-02  2.19967514e-02  2.56648939e-03 -1.33087458e-02\n",
      "   3.71040143e-02 -4.39287163e-03  3.11069074e-03 -1.98797900e-02\n",
      "  -1.95147283e-02  5.47865182e-02 -1.87021792e-02 -9.69310571e-03\n",
      "   4.98562716e-02  3.92557122e-02 -6.16777614e-02  2.56164428e-02\n",
      "   3.78633700e-02  2.06297357e-02  2.35545523e-02  2.46429257e-02\n",
      "   6.13273401e-03 -1.65693164e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17b792c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
