{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99abf36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import sklearn.utils\n",
    "import sklearn.preprocessing\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "def make_generator(nsteps):\n",
    "    generator = tf.keras.Sequential()\n",
    "    generator.add(tf.keras.layers.Dense(5, input_shape=(5,), activation='relu')) # 5\n",
    "    generator.add(tf.keras.layers.BatchNormalization())\n",
    "    generator.add(tf.keras.layers.Dense(10, activation='relu'))                  # 10\n",
    "    generator.add(tf.keras.layers.BatchNormalization())\n",
    "    generator.add(tf.keras.layers.Dense((5*nsteps), activation='relu'))          # 25\n",
    "    generator.add(tf.keras.layers.BatchNormalization())\n",
    "    generator.add(tf.keras.layers.Dense((5*nsteps), activation='tanh'))          # 25\n",
    "\n",
    "    return generator\n",
    "\n",
    "\n",
    "def make_critic(nsteps):\n",
    "    discriminator = tf.keras.Sequential()\n",
    "    discriminator.add(tf.keras.layers.Dense(5*nsteps, input_shape=(5*nsteps,)))\n",
    "    discriminator.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "    discriminator.add(tf.keras.layers.Dropout(0.3))\n",
    "    discriminator.add(tf.keras.layers.Dense(10))\n",
    "    discriminator.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "    discriminator.add(tf.keras.layers.Dense(5))\n",
    "    discriminator.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "    discriminator.add(tf.keras.layers.Dropout(0.3))\n",
    "    discriminator.add(tf.keras.layers.Flatten())\n",
    "    discriminator.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "\n",
    "    return discriminator\n",
    "\n",
    "\n",
    "def discriminator_loss(d_real, d_fake):\n",
    "    d_loss = tf.reduce_mean(d_fake) - tf.reduce_mean(d_real)\n",
    "    return d_loss\n",
    "\n",
    "def generator_loss(d_fake):\n",
    "    g_loss = -tf.reduce_mean(d_fake)\n",
    "    return g_loss\n",
    "\n",
    "@tf.function\n",
    "def train_step(noise, real, lmbda, n_critic, g, c, g_opt, c_opt, g1_loss, d1_loss, w_loss):   \n",
    "    batch_size = len(real)\n",
    "\n",
    "    ##### train critic ######\n",
    "    \n",
    "    #print('inside train step') \n",
    "\n",
    "    for i in range(n_critic):    \n",
    "        with tf.GradientTape() as t:\n",
    "            with tf.GradientTape() as t1:\n",
    "                fake = g(noise, training=True) # training=False?\n",
    "\n",
    "                epsilon = tf.random.uniform(shape=[batch_size, 1], minval=0., maxval=1.)\n",
    "                #print ('types', real.dtype, epsilon.dtype, fake.dtype)                \n",
    "                interpolated = real + epsilon * (fake - real) \n",
    "                t1.watch(interpolated)\n",
    "                c_inter = c(interpolated, training=True)  \n",
    "                d_real = c(real, training=True)\n",
    "                d_fake = c(fake, training=True)\n",
    "                d_loss = discriminator_loss(d_real, d_fake)   #initial c loss\n",
    "                \n",
    "                #print('d loop', d_real.shape,  d_fake.shape,  d_loss.shape, c_inter.shape) # batch_size by 1 except for d_loss\n",
    "                #print('d loop',  interpolated.shape, inpt.shape, real.shape, fake.shape) # \n",
    "                \n",
    "                \n",
    "                #print('discriminator loop',i ,'---------------------------------------------')    \n",
    "                #print('min and max values of fake',np.min(fake.numpy()),np.max(fake.numpy()))\n",
    "                #print('min and max values of real',np.min(real),np.max(real) )\n",
    "                #print('min and max values of d_fake',np.min(d_fake.numpy()),np.max(d_fake.numpy()))\n",
    "                #print('min and max values of d_real',np.min(d_real.numpy()),np.max(d_real.numpy()))\n",
    "                \n",
    "            grad_interpolated = t1.gradient(c_inter, interpolated)\n",
    "            \n",
    "            #print('interpolated      ', interpolated.numpy())\n",
    "            #print('c_inter           ', c_inter.numpy())\n",
    "            #print('grad_interpolated', grad_interpolated.numpy())\n",
    "            \n",
    "            #print('grad_interpolated itself ', grad_interpolated.numpy().shape) # batch_size by 25\n",
    "            #print('grad_interpolated square ', tf.square(grad_interpolated).numpy().shape) # batch_size by 25\n",
    "            #print('grad_interpolated red sum', tf.reduce_sum(tf.square(grad_interpolated), axis=[1]).numpy().shape) # batch_size by 1\n",
    "            #print('grad_interpolated sqrt', tf.sqrt(tf.reduce_sum(tf.square(grad_interpolated), axis=[1])).numpy().shape) # batch_size by 1            \n",
    "            \n",
    "                     #tf.sqrt(tf.reduce_sum(tf.square(x)) + 1.0e-12)\n",
    "            slopes = tf.sqrt(tf.reduce_sum(tf.square(grad_interpolated) + 1e-12, axis=[1])) # \n",
    "            \n",
    "            gradient_penalty = tf.reduce_mean((slopes - 1.) ** 2)\n",
    "            #print('slopes, grad penalty', slopes.numpy(), gradient_penalty.numpy())\n",
    "\n",
    "            new_d_loss = d_loss + (lmbda*gradient_penalty)  #new c loss\n",
    "            #print('d_loss and new_d_loss',d_loss.numpy(), new_d_loss.numpy()) \n",
    "        \n",
    "        c_grad = t.gradient(new_d_loss, c.trainable_variables)\n",
    "        #print('length and type c_grad', len(c_grad), type(c_grad))  # length of c_grad was 8? type list\n",
    "        #print('length and type c_grad[0]', (c_grad[0].shape), type(c_grad[0]))  # length of c_grad was 8? type list\n",
    "        #print('values of cgrad',c_grad)\n",
    "        #print('c.trainable_variables', c.trainable_variables)\n",
    "        c_opt.apply_gradients(zip(c_grad, c.trainable_variables))\n",
    "\n",
    "\n",
    "    ##### train generator #####\n",
    "\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        fake_images = g(noise, training=True)\n",
    "        d_fake = c(fake_images, training=True) # training=False?\n",
    "        g_loss = generator_loss(d_fake)\n",
    "\n",
    "    gen_grads = gen_tape.gradient(g_loss, g.trainable_variables)\n",
    "    g_opt.apply_gradients(zip(gen_grads, g.trainable_variables))\n",
    "    \n",
    "    \n",
    "    #print('g.trainable_variables')\n",
    "    #for v in g.trainable_variables:\n",
    "    #  print(v.name)\n",
    "    #print('c.trainable_variables')\n",
    "    #for v in c.trainable_variables:\n",
    "    #  print(v.name)\n",
    "\n",
    "    ### for tensorboard\n",
    "    g1_loss(g_loss)\n",
    "    d1_loss(new_d_loss)\n",
    "    w_loss((-1)*(d_loss))  #wasserstein distance\n",
    "\n",
    "\n",
    "    return \n",
    "\n",
    "def train(nsteps,ndims,lmbda,n_critic,batch_size,batches,training_data,input_to_GAN, epochs, g, c, g_opt, c_opt, g1_loss, d1_loss, w_loss, g1_summary_writer, d1_summary_writer, w_summary_writer):\n",
    "\n",
    "    losses = np.zeros((epochs,4))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        noise = input_to_GAN\n",
    "        real_data = training_data #X1.astype('int')\n",
    "\n",
    "        # uncommenting this line means that the noise is not paired with the outputs (probably desirable)\n",
    "        #noise = np.random.normal(size=[noise.shape[0],noise.shape[1]])\n",
    " \n",
    "        real_data, noise = sklearn.utils.shuffle(real_data, noise) #shuffle each epoch\n",
    "        #print ('shuffled l_input1 and xn1_comp')\n",
    "        \n",
    "        xx1 = real_data.reshape(batches, batch_size, ndims*nsteps)\n",
    "        inpt1 = noise.reshape(batches, batch_size, ndims)\n",
    "        #print ('data arranged in batches')\n",
    "        \n",
    "        print(g.layers[0].weights)\n",
    "\n",
    "        for i in range(len(xx1)):\n",
    "            #print('calling train_step', i ,'of',len(xx1), '-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*')  \n",
    "            train_step(inpt1[i], xx1[i], lmbda, n_critic, g, c, g_opt, c_opt, g1_loss, d1_loss, w_loss)\n",
    "            #print('back from train_step')  \n",
    "\n",
    "        print('epoch:', epoch, '*************************************************')    \n",
    "        print('gen loss', g1_loss.result().numpy(), 'd loss', d1_loss.result().numpy(), 'w_loss' , w_loss.result().numpy())\n",
    "\n",
    "        losses[epoch,:] = [ epoch+1, g1_loss.result().numpy() ,   d1_loss.result().numpy(),  w_loss.result().numpy()]\n",
    "\n",
    "        with g1_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', g1_loss.result(), step=epoch)\n",
    "\n",
    "        with d1_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', d1_loss.result(), step=epoch)\n",
    "\n",
    "        with w_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', w_loss.result(), step=epoch)\n",
    "            \n",
    "        #print('reset states')\n",
    "        g1_loss.reset_states()\n",
    "        d1_loss.reset_states()\n",
    "        w_loss.reset_states()\n",
    "\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "        #if epoch < 100 or (epoch + 1) % 100 == 0 :\n",
    "                       \n",
    "            saved_g1_dir = './saved_g_' + str(epoch + 1)\n",
    "            saved_d1_dir = './saved_c_' + str(epoch + 1)\n",
    "            tf.keras.models.save_model(g, saved_g1_dir)\n",
    "            tf.keras.models.save_model(c, saved_d1_dir)\n",
    "\n",
    "    np.savetxt('losses.csv', losses, delimiter=',')\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "def learn_hypersurface_from_POD_coeffs(input_to_GAN, training_data, nsteps, nPOD, ndims, lmbda, n_critic, batch_size, batches, ndims_latent_input):\n",
    "    # nPOD not needed\n",
    "\n",
    "    try:\n",
    "      print('looking for previous saved models')\n",
    "      saved_g1_dir = './saved_g_' + str(model_number)\n",
    "      g = tf.keras.models.load_model(saved_g1_dir)\n",
    "\n",
    "      saved_d1_dir = './saved_c_' + str(model_number)\n",
    "      c = tf.keras.models.load_model(saved_d1_dir)\n",
    "\n",
    "\n",
    "    except:\n",
    "      print('making new generator and critic')\n",
    "      g = make_generator(nsteps)\n",
    "      c = make_critic(nsteps)\n",
    "\n",
    "\n",
    "    g_opt = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0, beta_2=0.9)\n",
    "    c_opt = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0, beta_2=0.9)\n",
    "\n",
    "    g1_loss = tf.keras.metrics.Mean('g1_loss', dtype=tf.float32)\n",
    "    d1_loss = tf.keras.metrics.Mean('d1_loss', dtype=tf.float32)\n",
    "    w_loss = tf.keras.metrics.Mean('w_loss', dtype=tf.float32)\n",
    "\n",
    "    # logs to follow losses on tensorboard\n",
    "    print('initialising logs for tensorboard')\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    g1_log_dir = './logs/gradient_tape/' + current_time + '/g'\n",
    "    d1_log_dir = './logs/gradient_tape/' + current_time + '/d'\n",
    "    w_log_dir = './logs/gradient_tape/' + current_time + '/w'\n",
    "\n",
    "    g1_summary_writer = tf.summary.create_file_writer(g1_log_dir)\n",
    "    d1_summary_writer = tf.summary.create_file_writer(d1_log_dir)\n",
    "    w_summary_writer = tf.summary.create_file_writer(w_log_dir)\n",
    "\n",
    "\n",
    "    print('beginning training')\n",
    "    epochs = 500\n",
    "    generator = train(nsteps,ndims,lmbda,n_critic,batch_size,batches,training_data,input_to_GAN, epochs, g, c, g_opt, c_opt, g1_loss, d1_loss, w_loss, g1_summary_writer, d1_summary_writer, w_summary_writer)\n",
    "    print('ending training')\n",
    "\n",
    "\n",
    "    # generate some random inputs and put through generator\n",
    "    number_test_examples = 10\n",
    "    test_input = tf.random.normal([number_test_examples, ndims_latent_input])\n",
    "    predictions = generator(test_input, training=False)\n",
    "    #predictions = generator.predict(test_input) # number_test_examples by ndims_latent_input\n",
    "\n",
    "    predictions_np = predictions.numpy() # nExamples by nPOD*nsteps\n",
    "    #tf.compat.v1.InteractiveSession()\n",
    "    #predictions_np = predictions.numpy().\n",
    "    print('Shape of the output of the GAN', predictions_np.shape)\n",
    "    predictions_np = predictions_np.reshape(number_test_examples*nsteps, nPOD)\n",
    "    print('Reshaping the GAN output (in order to apply inverse scaling)', predictions_np.shape)\n",
    "\n",
    "    return predictions_np, generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5be9ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in the POD coeffs.\n",
      "type and shape (nPOD by nTrain) of POD coeffs from csv file <class 'numpy.ndarray'> (5, 204) float32\n",
      "shape csv data for the scaling (204, 5)\n",
      "min and max of col,  0  of csv_data: -1.0 1.0\n",
      "min and max of col,  1  of csv_data: -1.0 1.0000001\n",
      "min and max of col,  2  of csv_data: -1.0 1.0\n",
      "min and max of col,  3  of csv_data: -1.0 1.0\n",
      "min and max of col,  4  of csv_data: -1.0 1.0\n",
      "Shape of training data for the GAN (200, 25) float32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "# reproducibility\n",
    "import random\n",
    "\n",
    "np.random.seed(143)\n",
    "random.seed(143)\n",
    "tf.random.set_seed(143)\n",
    "\n",
    "# read in data, reshape and normalise\n",
    "lmbda = 10\n",
    "n_critic = 5\n",
    "\n",
    "batch_size = 20 # 32  \n",
    "batches = 10    # 900 \n",
    "\n",
    "ndims_latent_input = 5 # 128 # latent variables for GAN\n",
    "\n",
    "# data settings\n",
    "nsteps = 5  #number of consecutive timesteps in gan\n",
    "ndims = 5 # == nPOD # 128 # reduced variables i.e. POD coefficients or AE latent variables\n",
    "\n",
    "# reading in the data\n",
    "print('Reading in the POD coeffs.')\n",
    "#csv_data = np.loadtxt('/kaggle/input/fpc-204examples-5pod-without-ic/POD_coeffs_1_204.csv', delimiter=',')\n",
    "csv_data = np.loadtxt('../data/processed/POD_coeffs_1_204_orig.csv', delimiter=',')\n",
    "csv_data = np.float32(csv_data)\n",
    "print('type and shape (nPOD by nTrain) of POD coeffs from csv file', type(csv_data), csv_data.shape, csv_data.dtype)\n",
    "\n",
    "nTrain = csv_data.shape[1]\n",
    "nPOD = csv_data.shape[0]\n",
    "\n",
    "csv_data = csv_data.T # nTrain by nPOD\n",
    "\n",
    "# scaling the POD coeffs\n",
    "scaling = sklearn.preprocessing.MinMaxScaler(feature_range=[-1,1])\n",
    "print('shape csv data for the scaling', csv_data.shape) \n",
    "csv_data = scaling.fit_transform(csv_data)\n",
    "\n",
    "# check that the columns are scaled between min and max values (-1,1)\n",
    "for icol in range(csv_data.shape[1]):\n",
    "    print('min and max of col, ', icol ,' of csv_data:', np.min(csv_data[:,icol]), np.max(csv_data[:,icol]) )\n",
    "\n",
    "# create nsteps time levels for the training_data for the GAN\n",
    "t_begin = 0\n",
    "t_end = nTrain - nsteps + 1\n",
    "training_data = np.zeros((t_end,nPOD*nsteps),dtype=np.float32) # nTrain by nsteps*nPOD # 'float32' or np.float32\n",
    "\n",
    "for step in range(nsteps):\n",
    "    #print ('training data - cols',step*nPOD,'to',(step+1)*nPOD )\n",
    "    #print ('csv data - rows', t_begin+step ,'to', t_end+step )\n",
    "    training_data[:,step*nPOD:(step+1)*nPOD] = csv_data[t_begin+step : t_end+step,:]\n",
    "\n",
    "print('Shape of training data for the GAN', training_data.shape, training_data.dtype)\n",
    "\n",
    "# GAN input\n",
    "try:\n",
    "    input_to_GAN = np.load('input_to_GAN.npy')\n",
    "except:\n",
    "    input_to_GAN = tf.random.normal([training_data.shape[0], ndims_latent_input])\n",
    "    input_to_GAN = input_to_GAN.numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9cf0de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking for previous saved models\n",
      "making new generator and critic\n",
      "initialising logs for tensorboard\n",
      "beginning training\n",
      "[<tf.Variable 'dense/kernel:0' shape=(5, 5) dtype=float32, numpy=\n",
      "array([[ 0.36947453, -0.56368136,  0.42505252, -0.52954215, -0.3562048 ],\n",
      "       [-0.09366822, -0.6652722 , -0.49555767,  0.11435276,  0.3177513 ],\n",
      "       [ 0.35808337,  0.13187635, -0.26672226,  0.4327829 ,  0.251755  ],\n",
      "       [-0.6485403 ,  0.14122897, -0.3009151 , -0.6499302 ,  0.4023627 ],\n",
      "       [-0.20274329, -0.22054869,  0.4586861 ,  0.33503056,  0.45004296]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(5,) dtype=float32, numpy=array([0., 0., 0., 0., 0.], dtype=float32)>]\n",
      "epoch: 0 *************************************************\n",
      "gen loss 0.5165217 d loss 5.405176 w_loss -0.20684215\n",
      "[<tf.Variable 'dense/kernel:0' shape=(5, 5) dtype=float32, numpy=\n",
      "array([[ 0.36983585, -0.563494  ,  0.4244618 , -0.5294896 , -0.35570318],\n",
      "       [-0.09364009, -0.6654967 , -0.49585912,  0.11403979,  0.31687245],\n",
      "       [ 0.35873526,  0.13224989, -0.2668667 ,  0.43269202,  0.25209054],\n",
      "       [-0.6481997 ,  0.14130561, -0.3012748 , -0.65021616,  0.40263495],\n",
      "       [-0.2026327 , -0.22038855,  0.458512  ,  0.33493322,  0.45020106]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(5,) dtype=float32, numpy=\n",
      "array([-9.1016278e-05, -4.9059535e-04, -1.3118502e-04,  5.5105054e-05,\n",
      "        3.1488435e-04], dtype=float32)>]\n",
      "epoch: 1 *************************************************\n",
      "gen loss 0.4312807 d loss 4.435807 w_loss -0.15107669\n",
      "[<tf.Variable 'dense/kernel:0' shape=(5, 5) dtype=float32, numpy=\n",
      "array([[ 0.37043154, -0.56332016,  0.42451948, -0.5300138 , -0.35607797],\n",
      "       [-0.09372456, -0.6659332 , -0.49606454,  0.11383728,  0.31639838],\n",
      "       [ 0.35853276,  0.13235761, -0.26686862,  0.43237355,  0.25163636],\n",
      "       [-0.6479902 ,  0.14111438, -0.30113465, -0.65078074,  0.4024529 ],\n",
      "       [-0.2024416 , -0.22003263,  0.4582897 ,  0.33419168,  0.4506984 ]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(5,) dtype=float32, numpy=\n",
      "array([ 8.6022410e-06, -6.8145205e-04, -1.9772940e-04,  1.9243536e-04,\n",
      "        2.7863201e-04], dtype=float32)>]\n",
      "epoch: 2 *************************************************\n",
      "gen loss 0.43180338 d loss 3.7514966 w_loss -0.11454954\n",
      "[<tf.Variable 'dense/kernel:0' shape=(5, 5) dtype=float32, numpy=\n",
      "array([[ 0.37064204, -0.5629514 ,  0.42452085, -0.5296428 , -0.35637125],\n",
      "       [-0.09393353, -0.66624796, -0.49616423,  0.11376473,  0.31631902],\n",
      "       [ 0.35879764,  0.13225196, -0.26671302,  0.4323056 ,  0.25175294],\n",
      "       [-0.6475162 ,  0.14135674, -0.30097082, -0.6514086 ,  0.4021988 ],\n",
      "       [-0.2025932 , -0.2200986 ,  0.45842606,  0.33398548,  0.45053965]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(5,) dtype=float32, numpy=\n",
      "array([ 1.8124553e-04, -1.2430392e-03,  4.9715956e-05,  5.5208446e-05,\n",
      "        5.6915160e-04], dtype=float32)>]\n",
      "epoch: 3 *************************************************\n",
      "gen loss 0.39311004 d loss 2.9157052 w_loss -0.047988903\n",
      "[<tf.Variable 'dense/kernel:0' shape=(5, 5) dtype=float32, numpy=\n",
      "array([[ 0.37090623, -0.5630341 ,  0.42467666, -0.5292342 , -0.3559453 ],\n",
      "       [-0.09393704, -0.6661945 , -0.496068  ,  0.11340538,  0.31675223],\n",
      "       [ 0.35932484,  0.13222916, -0.26659086,  0.43305847,  0.25166282],\n",
      "       [-0.6469444 ,  0.14185473, -0.30086428, -0.65157294,  0.4016305 ],\n",
      "       [-0.20264281, -0.21980092,  0.45856053,  0.33369368,  0.4510023 ]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(5,) dtype=float32, numpy=\n",
      "array([ 5.7585549e-04, -1.3683655e-03, -2.1071624e-04, -6.7203655e-07,\n",
      "        1.3688672e-04], dtype=float32)>]\n",
      "epoch: 4 *************************************************\n",
      "gen loss 0.41618714 d loss 2.924019 w_loss -0.057913255\n",
      "[<tf.Variable 'dense/kernel:0' shape=(5, 5) dtype=float32, numpy=\n",
      "array([[ 0.37012565, -0.5625944 ,  0.42481792, -0.52889264, -0.35581148],\n",
      "       [-0.09413623, -0.6667261 , -0.49605554,  0.11296215,  0.3170009 ],\n",
      "       [ 0.35945243,  0.13246416, -0.26691473,  0.43336853,  0.2516758 ],\n",
      "       [-0.64717317,  0.14179745, -0.30068362, -0.6518331 ,  0.40189844],\n",
      "       [-0.20280762, -0.21935181,  0.4583687 ,  0.33367217,  0.45068112]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(5,) dtype=float32, numpy=\n",
      "array([-0.00012032, -0.00146598, -0.00029756, -0.00032123, -0.00018168],\n",
      "      dtype=float32)>]\n",
      "epoch: 5 *************************************************\n",
      "gen loss 0.40035662 d loss 2.7703867 w_loss -0.029803758\n",
      "[<tf.Variable 'dense/kernel:0' shape=(5, 5) dtype=float32, numpy=\n",
      "array([[ 0.37000942, -0.56260276,  0.42460498, -0.52912825, -0.3555115 ],\n",
      "       [-0.0943651 , -0.66673553, -0.49645364,  0.11281408,  0.31710443],\n",
      "       [ 0.35944095,  0.1330563 , -0.26702666,  0.4331409 ,  0.25205046],\n",
      "       [-0.6473694 ,  0.14172582, -0.30024886, -0.65203774,  0.4014838 ],\n",
      "       [-0.20254582, -0.21895328,  0.45848706,  0.33346075,  0.45079818]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(5,) dtype=float32, numpy=\n",
      "array([-0.00040596, -0.00193915, -0.00048408, -0.0008383 , -0.00023909],\n",
      "      dtype=float32)>]\n",
      "epoch: 6 *************************************************\n",
      "gen loss 0.37698555 d loss 2.2102027 w_loss 0.055111904\n",
      "[<tf.Variable 'dense/kernel:0' shape=(5, 5) dtype=float32, numpy=\n",
      "array([[ 0.37009153, -0.5626607 ,  0.42494515, -0.52947444, -0.35559833],\n",
      "       [-0.09453273, -0.6668844 , -0.4962467 ,  0.11239947,  0.3172886 ],\n",
      "       [ 0.3597476 ,  0.13327575, -0.26717666,  0.43296075,  0.25177336],\n",
      "       [-0.64722204,  0.14150377, -0.30024514, -0.6517853 ,  0.40141752],\n",
      "       [-0.20202377, -0.21853755,  0.4581817 ,  0.33338603,  0.45089123]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(5,) dtype=float32, numpy=\n",
      "array([-0.00012848, -0.00221664, -0.00066782, -0.00131371, -0.00053581],\n",
      "      dtype=float32)>]\n",
      "epoch: 7 *************************************************\n",
      "gen loss 0.3379846 d loss 2.092153 w_loss 0.07770256\n",
      "[<tf.Variable 'dense/kernel:0' shape=(5, 5) dtype=float32, numpy=\n",
      "array([[ 0.37059158, -0.5622277 ,  0.42478848, -0.52923214, -0.35567042],\n",
      "       [-0.09484472, -0.6671974 , -0.49571434,  0.11249373,  0.31703433],\n",
      "       [ 0.36017144,  0.13377802, -0.2676279 ,  0.43310148,  0.2519788 ],\n",
      "       [-0.6462715 ,  0.14164247, -0.30006972, -0.6516428 ,  0.40191847],\n",
      "       [-0.20220472, -0.21831714,  0.45853347,  0.33377606,  0.45058972]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(5,) dtype=float32, numpy=\n",
      "array([-2.1257740e-05, -2.4101287e-03, -7.4006122e-04, -1.0888191e-03,\n",
      "       -7.6428417e-04], dtype=float32)>]\n",
      "epoch: 8 *************************************************\n",
      "gen loss 0.37309393 d loss 2.515603 w_loss 0.094309315\n",
      "[<tf.Variable 'dense/kernel:0' shape=(5, 5) dtype=float32, numpy=\n",
      "array([[ 0.37036717, -0.561778  ,  0.42435312, -0.5288253 , -0.35608706],\n",
      "       [-0.09469518, -0.66747946, -0.4959565 ,  0.11232907,  0.31669864],\n",
      "       [ 0.3600836 ,  0.13445197, -0.26792976,  0.43361267,  0.25175977],\n",
      "       [-0.6465284 ,  0.14171135, -0.29983646, -0.6518809 ,  0.40244025],\n",
      "       [-0.20211597, -0.21802808,  0.45846307,  0.33350423,  0.45044687]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(5,) dtype=float32, numpy=\n",
      "array([-0.00025694, -0.0032606 , -0.00082165, -0.00127044, -0.00048471],\n",
      "      dtype=float32)>]\n",
      "epoch: 9 *************************************************\n",
      "gen loss 0.3599196 d loss 2.3382783 w_loss 0.116460815\n",
      "[<tf.Variable 'dense/kernel:0' shape=(5, 5) dtype=float32, numpy=\n",
      "array([[ 0.37022522, -0.5614732 ,  0.42429394, -0.5292225 , -0.35569546],\n",
      "       [-0.09426025, -0.66765577, -0.49645168,  0.11222076,  0.31667703],\n",
      "       [ 0.36022913,  0.13455786, -0.26754937,  0.43331048,  0.2521101 ],\n",
      "       [-0.64669   ,  0.14227049, -0.30031568, -0.6517937 ,  0.40219963],\n",
      "       [-0.20219065, -0.21786804,  0.45797548,  0.3335352 ,  0.45065504]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(5,) dtype=float32, numpy=\n",
      "array([ 5.41074187e-05, -3.75822769e-03, -1.12836529e-03, -1.60165364e-03,\n",
      "       -1.08716464e-04], dtype=float32)>]\n",
      "epoch: 10 *************************************************\n",
      "gen loss 0.37773493 d loss 2.2829351 w_loss 0.22690599\n",
      "[<tf.Variable 'dense/kernel:0' shape=(5, 5) dtype=float32, numpy=\n",
      "array([[ 0.37027785, -0.56169325,  0.42428038, -0.5293628 , -0.35606274],\n",
      "       [-0.09384976, -0.6676689 , -0.49652714,  0.11179005,  0.31705737],\n",
      "       [ 0.36024547,  0.13416922, -0.26756367,  0.43277362,  0.2519762 ],\n",
      "       [-0.64657027,  0.14211135, -0.29980978, -0.6523769 ,  0.4018228 ],\n",
      "       [-0.20245501, -0.21773024,  0.45816663,  0.33348557,  0.45032367]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(5,) dtype=float32, numpy=\n",
      "array([-0.00030901, -0.00416821, -0.0009981 , -0.00225659,  0.00024635],\n",
      "      dtype=float32)>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 *************************************************\n",
      "gen loss 0.39308327 d loss 2.5052605 w_loss 0.19742014\n",
      "[<tf.Variable 'dense/kernel:0' shape=(5, 5) dtype=float32, numpy=\n",
      "array([[ 0.36996797, -0.56202453,  0.4241141 , -0.5294824 , -0.35581687],\n",
      "       [-0.09372365, -0.66739917, -0.49686006,  0.11142189,  0.31743345],\n",
      "       [ 0.3608952 ,  0.13464211, -0.26752752,  0.43261847,  0.25207824],\n",
      "       [-0.64642143,  0.14232738, -0.29954723, -0.65293723,  0.40147886],\n",
      "       [-0.20197636, -0.2172606 ,  0.45820117,  0.3331667 ,  0.45036468]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(5,) dtype=float32, numpy=\n",
      "array([-0.00011725, -0.00448398, -0.00075781, -0.00268445,  0.00027848],\n",
      "      dtype=float32)>]\n",
      "epoch: 12 *************************************************\n",
      "gen loss 0.45525533 d loss 2.2407117 w_loss 0.23892185\n",
      "[<tf.Variable 'dense/kernel:0' shape=(5, 5) dtype=float32, numpy=\n",
      "array([[ 0.37017402, -0.5623975 ,  0.42436108, -0.52967745, -0.35544986],\n",
      "       [-0.09329385, -0.6670796 , -0.49690747,  0.11137967,  0.3173499 ],\n",
      "       [ 0.3611085 ,  0.13497505, -0.26735398,  0.4327829 ,  0.2523343 ],\n",
      "       [-0.6463798 ,  0.14228772, -0.29961264, -0.65261257,  0.40130305],\n",
      "       [-0.20143336, -0.21711622,  0.45803562,  0.33299315,  0.45076844]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(5,) dtype=float32, numpy=\n",
      "array([-0.00033278, -0.00447372, -0.00065289, -0.00241969,  0.00054023],\n",
      "      dtype=float32)>]\n",
      "epoch: 13 *************************************************\n",
      "gen loss 0.46133405 d loss 2.01544 w_loss 0.2919186\n",
      "[<tf.Variable 'dense/kernel:0' shape=(5, 5) dtype=float32, numpy=\n",
      "array([[ 0.37017676, -0.5626613 ,  0.42469844, -0.52988565, -0.35538515],\n",
      "       [-0.09340741, -0.6669487 , -0.49672613,  0.11143198,  0.3170582 ],\n",
      "       [ 0.36178792,  0.1351867 , -0.26757962,  0.43272576,  0.2528709 ],\n",
      "       [-0.64595354,  0.14157656, -0.30016518, -0.652509  ,  0.40135968],\n",
      "       [-0.20104712, -0.21728352,  0.45734024,  0.33288142,  0.45055735]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(5,) dtype=float32, numpy=\n",
      "array([-0.00015863, -0.00498516, -0.00035229, -0.00247204,  0.00044798],\n",
      "      dtype=float32)>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "predictions, generator = learn_hypersurface_from_POD_coeffs(input_to_GAN, training_data, nsteps, nPOD, ndims, lmbda, n_critic, batch_size, batches, ndims_latent_input)\n",
    "t_train = time.time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7696ef7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
